<h1>Uzaklıklar, Norm, Benzerlik</h1>
<!DOCTYPE html>
<html>
  <head>
    <title>Uzaklıklar, Norm, Benzerlik
</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [["$","$"]]}
      });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    </script>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full">
</script>
</head>

<p>Literatürdeki anlatım norm ve uzaklık konusu etrafında biraz kafa karışıklığı
yaratabiliyor, bu yazıda biraz açıklık getirmeye çalışalım. Norm bir büyüklük
ölçüsüdür. Vektör uzayları ile olan alakasını görmek için {\em Fonksiyonel
  Analiz} notlarına bakılabilir. Büyüklük derken bir $x$ vektörünün
büyüklüğünden bahsediyoruz, ki bu çoğunlukla $||x||$ gibi bir kullanımda
görülür, eğer altsimge yok ise, o zaman 2 kabul edilir, yani $||x||_2$. Bu ifade
bir L2 norm'unu ifade eder. $||x||_1$ varsa L1 norm'ü olurdu.</p>
<p>L1,L2 normaları, ya da genel olarak $p$ üzerinden $L_p$ normları şöyle gösterilir,</p>
<p>$$ ||x||_p = (\sum_i |x_i|^p)^{1/p} $$</p>
<p>ki $x_i$, $x$ vektörü içindeki öğelerdir. Eğer $p=2$ ise, L2 norm</p>
<p>$$ ||x||_2 = \bigg(\sum_i |x_i|^2 \bigg)^{1/2} $$</p>
<p>Üstel olarak $1/2$'nin karekök demek olduğunu hatırlayalım, yani </p>
<p>$$ ||x||_2 = \sqrt{\sum |x_i|^2} $$</p>
<p>Bu norm ayrıca Öklitsel (Euclidian) norm olarak ta bilinir, tabii ki bunun
Öklitsel uzaklık ile yakın bağlantısı var (iki vektörü birbirinden çıkartıp
Öklit normunu alırsak Öklit uzaklığını hesaplamış oluruz).</p>
<p>Eğer $p=1$ olsaydı, yani L1 norm, o zaman üstel olarak $1/1$ olur, yani hiçbir
üstel / köksel işlem yapılmasına gerek yoktur, iptal olurlar,</p>
<p>$$ ||x||_1 = \sum |x_i|^2 $$</p>
<p>Örnek</p>
<p>$$ 
a = \left[\begin{array}{r}
3 \\ -2 \\ 1
\end{array}\right]
 $$</p>
<p>$$ ||a|| = \sqrt{3^2+(-2)^2+1^2} = 3.742 $$</p>
<p>Örnekte altsimge yok, demek ki L2 norm. </p>
<p>Ek Notasyon, İşlemler</p>
<p>L1 normu için yapılan işlemi düşünelim, vektör öğeleri kendileri ile
çarpılıyor ve sonuçlar toplanıyor. Bu işlem</p>
<p>$||x||_1 = x^Tx$</p>
<p>olarak ta gösterilemez mi? Ya da $x \cdot x$ olarak ki bu noktasal çarpımdır.</p>
<p>Bazen de yapay öğrenim literatüründe $||x||^2$ şekilde bir kullanım
görebiliyorsunuz. Burada neler oluyor? Altsimge yok, demek ki L2
norm. Sonra L2 normun karesi alınmış, fakat L2 normu tanımına göre bir
karekök almıyor muydu? Evet, fakat o zaman kare işlemi karekökü iptal eder,
demek ki L2 normunun karesini almak bizi L1 normuna döndürür! Eh bu normu
da $x^Tx$ olarak hesaplayabildiğimize göre hemen o notasyona geçebiliriz,
demek ki $||x||^2 = x^Tx = x \cdot x$. </p>
<p>İkisel Vektörlerde Benzerlik</p>
<p>Diğer ilginç bir kullanım ikisel değerler içeren iki vektör arasında
çakışan 1 değerlerinin toplamını bulmak. Mesela </p>
<pre><code class="python">a = np.array([1,0,0,1,0,0,1,1])
b = np.array([0,0,1,1,0,1,1,0])
</code></pre>

<p>Bu iki vektör arasındaki 1 uyusumunu bulmak için noktasal çarpım yeterli,
çünkü 1 ve 0, 0 ve 1, 0 ve 0 çarpımı sıfır verir, ama 1 çarpı 1 = 1
sonucunu verir. O zaman L1 norm bize ikisel iki vektör arasında kabaca bir
benzerlik fikri verebilir.</p>
<pre><code class="python">print np.dot(a,b)
</code></pre>

<pre><code>2
</code></pre>

<p>Matris Normları</p>
<p>Vektörlerin norm'ü hesaplanabildiği gibi matris norm'ü da hesaplanabilir. Bir
$A$ matrisi için matris norm'ü</p>
<p>$$ || A || = \sup { ||Ax|| : x \in \mathbb{R}^n, ||x||=1 \textrm{ olacak şekilde } } $$</p>
<p>Bazen şöyle de gösterilir,</p>
<p>$$  || A || = \sup_{||x||=1} { ||Ax|| } $$</p>
<p>ya da</p>
<p>$$ || A || = \sup { \frac{||Ax||}{||x||} : x \in \mathbb{R}^n, x \ne 0 \textrm{ olacak şekilde } } $$</p>
<p>Daha genel formda p-norm'u</p>
<p>$$ || A || = \sup
\bigg\{
\frac{||Ax||_p}{||x||_p} : x \in \mathbb{R}^n, x \ne 0 \textrm{ olacak şekilde }
\bigg\} $$</p>
<p>Özel durum $p=2$ için ki bu yine, vektörler için olduğu gibi, Öklitsel norm
olarak biliniyor. Bu durumda $A$'nın normu $A$'nın en büyük eşsiz
değeridir. Yaklaşık olarak hesaplama açısından şunu da verelim,</p>
<p>$$ ||A||_1 = \max_{1 \le j \le n} \sum _{i=1}^{m} |a_{ij}| $$</p>
<p>Yani tüm matris kolonlarının hücrelerinin mutlak değerleri toplanıyor, bu
toplamlar arasında en büyük sayıyı veren kolonun toplamı normun yaklaşık
değeridir.</p>
<p>Spektral (Operatör) ve İz (Trace) Norm</p>
<p>Bu normlar sırasıyla matrisin en büyük eşsiz (singular) değeri, ve tüm
eşsiz değerlerinin toplamıyla hesaplanır. Bir matris $X$ için operatör norm</p>
<p>$$
||X||_{op} = \sigma_1(X)
$$</p>
<p>İz normu</p>
<p>$$
||X||_{tr} = \sigma _{i=1}^{r} \sigma_i(X)
$$</p>
<hr>

<p>$$ (x-v)^TA(x-v) &lt; 1 $$</p>
<p>Üstteki formülde $x$ yerine $Px$ geçirirsek, ki $P$ herhangi bir matris,
eşitsizliğin sol tarafına ne olur?</p>
<p>$$ (P(x-v))^T A (P(x-v))$$</p>
<p>$$ (x-v)^T P^T A P (x-v)  $$</p>
<p>Bu formüle bir şekilde ulaşmamız lazım. Ama nasıl? Basitleştirme amaçlı olarak
$w = x-v$ tanımlayalım, ki $x \ne v$ olacak şekilde. $X = \frac{1}{||w||^2} I$
tanımlayalım, bu bir köşegen matris, köşegeninde $1/||w||^2$ değerleri var. Bu
sayede</p>
<p>$$ w^T A W &lt; 1  \Rightarrow w^T A W &lt; w^T X w  $$</p>
<p>1 yerine üstteki en sağdaki terimi kullanmış olduk. Herhangi bir $x$ için
üstteki eşitsizlik her $w$ için doğru olacaktır. Bu da $A - X$ negatif kesin
demektir (pozitif kesinliğin tersi), o zaman şunu da söyleyebiliriz,</p>
<p>$$ A - X &lt; 0 \Rightarrow P^T(A-x)P &lt; 0 \Rightarrow P^T AP &lt; P^TXP $$</p>
<p>Soldan ve sağdan $w^T,w$ ile çarparsak,</p>
<p>$$ w^T P^T AP w &lt; w^T P^TXP w = \frac{1}{||w||^2} w^T P^T P w = (Pu)^T Pu$$</p>
<p>ki $u = \frac{w}{||w||}$ $x-v$ yönünü gösteren birim vektördür. </p>
<p>Şimdi matris normunun ne olduğunu hatırlayalım,</p>
<p>$$ ||P|| = \sup_{||u||=1} || Pu || $$</p>
<p>O zaman emin bir şekilde diyebiliriz ki </p>
<p>$$ (x-v)^TA(x-v) &lt; 1 \Rightarrow (x-v)^T P^T A P (x-v) &lt; ||P||^2 $$</p>
<hr>

<p>Sherley-Morrison Formülü</p>
<p>Bu formülün temeli şu eşitlikten başlıyor [1, sf. 124],</p>
<p>$$
(I+cd^T)^{-1} = I - \frac{cd^T}{1+d^Tc}
\qquad (1)
$$</p>
<p>ki $c,d$ birer vektör, ve $1+d^Tc \ne 0$ olacak şekilde, üstteki eşitliğin
doğru olduğunu kontrol için iki tarafı $(I+cd^T)$ ile çarpabiliriz, eğer
sağ tarafta birim matrisi elde edersek eşitlik doğru demektir,</p>
<p>$$
I + cd^T - \frac{cd^T (I + cd^T)}{1+d^Tc}
$$</p>
<p>$$
= I + cd^T - \frac{I cd^T (1+cd^T)}{1+d^Tc}
$$</p>
<p>$$
= I + cd^T - cd^T = I
$$</p>
<p>Eğer sıfırdan başlayarak türetmek istesek, öyle bir $\alpha$ arıyoruz ki
$(I + cd^T)$ ifadesini $(I + \alpha cd^T)$ ile çarpınca bize birim matrisi
versin. Çarpımı yaparsak,</p>
<p>$$
(I + cd^T) (I + \alpha cd^T) = I + cd^T + \alpha cd^T + \alpha cd^Tcd^T 
$$</p>
<p>$$
= I + (1 + \alpha + \alpha d^T c) cd^T
$$</p>
<p>Üsttekinin birim matrisi $I$ olması için $1 + \alpha + \alpha d^T c$ sıfır
olmalı, onun sıfır olması için de</p>
<p>$$\alpha = \frac{-1}{1 + d^Tc}$$</p>
<p>doğru olmalı. $\alpha$'yi yerine koyarsak, </p>
<p>$$
(I + \alpha cd^T) = I - \frac{cd^T}{1+d^Tc}
$$</p>
<p>elde ederiz, yani $(I + \alpha cd^T)^{-1}$ açılımı budur. </p>
<p>Sherman-Morrison formülü </p>
<p>$$
(A + cd^T)^{-1} = A^{-1} - \frac{A^{-1} cd^T A^{-1} }{1 + d^TA^{-1}c}
$$</p>
<p>Bu formüle erişmek için $(A + cd^T)^{-1}$ ile başlayalım, $A$'yi parantez
dışına çekersek,</p>
<p>$$
(A + cd^T)^{-1} = \big( A ( I + A ^{-1} cd^T ) \big)^{-1} 
$$</p>
<p>$$
= ( I + A ^{-1} cd^T )^{-1} A^{-1} 
$$</p>
<p>Parantez içinin (1)'in sol tarafına benzediğini görebiliriz, $b = A^{-1}c$ 
desek,  $( I + bd^T )^{-1} $ açılımıni yapıyor olurduk, </p>
<p>$$
( I + bd^T )^{-1} 
= I - \frac{bd^T}{1+d^Tb} 
= I - \frac{A^{-1}cd^T}{1+d^TA^{-1}c}
$$</p>
<p>Bu sonucu iki üstteki parantez içindeki $A ( I + A ^{-1} cd^T$ yerine
koyarsak,</p>
<p>$$
(A + cd^T)^{-1} = I - \frac{A^{-1}cd^T}{1+d^TA^{-1}c} A^{-1} 
$$</p>
<p>sonucuna erişmiş oluyoruz. </p>
<p>Sherman-Morrison-Woodburry</p>
<p>Bu son formül Sherman-Morrison formülünün daha genelleştirilmiş hali
[3]. Diyelim ki $A \in \mathbb{R}^{n \times n}$ eşsiz değil, ve
$U,V \in \mathbb{R}^{n \times p}$ öyle ki </p>
<p>$$
U + V^T A^{-1} U \in \mathbb{R}^{p \times p}
$$</p>
<p>O zaman </p>
<p>$$B = A + UV^T$$</p>
<p>eşsiz değildir, ve </p>
<p>$$
B^{-1} = A^{-1} - A^{-1} U ( I + V^T A^{-1} U)^{-1} V^T A^{-1} 
$$</p>
<p>Kaynaklar </p>
<p>[1] Meyer, <em>Matrix Analysis and Applied Linear Algebra</em></p>
<p>[2] Gadzinski, {How to Derive the Sherman-Morrison Base Formula, Math Stackexchange Sorusuna Cevap}, 
    <a href="https://math.stackexchange.com/a/3462542/6786">https://math.stackexchange.com/a/3462542/6786</a></p>
<p>[3] Gockenbach, <em>Numerical Optimization MA 5630, Globalizing Newton's method: Descent Directions (II)</em>
    <a href="https://pages.mtu.edu/~msgocken/ma5630spring2003/lectures.html">https://pages.mtu.edu/~msgocken/ma5630spring2003/lectures.html</a></p>
<p>[4] Marmer, <em>Economics 627 Econometric Theory II, Vector and Matrix Differentiation</em>, 
    <a href="http://faculty.arts.ubc.ca/vmarmer/econ627/">http://faculty.arts.ubc.ca/vmarmer/econ627/</a></p>
<p>[5] Duda, Hart, <em>Pattern Classification</em></p>
<p>[6] Bishop, <em>Pattern Recognition and Machine Learning</em></p>
<p>[7] Wikipedia, <em>Matrix norm</em>, 
    <a href="https://en.wikipedia.org/wiki/Matrix_norm">https://en.wikipedia.org/wiki/Matrix_norm</a></p>
<p>[8] Thibshirani, <em>Convex Optimization</em>, 
    <a href="https://www.stat.cmu.edu/~ryantibs/convexopt">https://www.stat.cmu.edu/~ryantibs/convexopt</a></p>
<hr>

<p>Yunan Harfleri</p>
<p><img alt="" src="../../algs/algs_999_zapp/letters.png" /></p>