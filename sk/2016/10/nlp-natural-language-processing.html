
<!DOCTYPE html>
<html>
  <head>
    <title></title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [["$","$"]]}
      });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    </script>
   <script type="text/javascript"
       src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full">
   </script>
   <script async="async" data-cfasync="false" src="//pl22489825.profitablegatecpm.com/d84f574876e65b2d8f0c7bae784c22b3/invoke.js"></script>

   <link rel="stylesheet" type="text/css" media="screen" href="https://burakbayramli.github.io/css/style.css">
  </head>
    <body>
      <div id="header_wrap" class="outer">
        <header class="inner">
          <h1 id="project_title">
            <a href="https://burakbayramli.github.io" style="text-decoration:none; color:inherit;">dersblog</a>
          </h1>
          <h2 id="project_tagline"></h2>          
        </header>
      </div>
      <div id="main_content_wrap" class="outer">        
        <section id="main_content" class="inner">
        <h1></h1>
<h1>Derin Öğrenme ile Doğal Dil İşlemek (Natural Language Processing -NLP-)</h1>

<p>Doküman sınıflamak, bir film için yazılmış yorumu beğendi / beğenmedi
şeklinde irdelemek; tüm bu işlemler doğal dil işlemek kategorisine girer,
ve derin yapay sinir ağları (DYSA) bu alanda kullanılabilir.</p>

<p>Doküman nasıl temsil edilir? </p>

<p>Doküman kelimelerden oluşur, fakat kelimeler sayısal değil kategorik
şeyler, DYSA kullanmak için kelimelerin sayısallaştırılması lazım. Bir
çözüm 1-hot kodlaması, tüm dokümanlardaki tüm kelimeler on bin kelimelik
bir "sözlükten'' geliyorsa, her kelime için on bin boyutunda bir vektör
yaratırız, bu vektörde kelimelerin yerleri önceden bellidir, "cat (kedi)''
kelimesi mesela 300. indis, o zaman "cat'' kelimesini temsil için 10,000
büyüklüğündeki bir vektörün 300. öğesi 1 diğer 9999 ögesi 0 olur.</p>

<p>Bu temsil şekli biraz israflı degil mi? Ayrıca kelimeler arasında benzerlik
için bize hiçbir fayda getirmiyor.</p>

<p>Daha önce [4] yazısında boyut azaltma işleminden bahsettik. Bir kelimeyi,
ya da dökümanı her ikisinin ilişkisini içeren bir matris üzerinde SVD
işlettikten sonra daha ufak bir boyutta temsil edebiliyorduk. Bu azaltılmış
boyutta, ki boyutu binlere varan ham veri için azaltılmış boyut
$k=10,20,100$ gibi olabiliyordu, kelimeler pür sayısal hale geliyordu ve
kelimelere tekabül eden $k$ büyüklüğündeki vektörlerin, anlamsal bağlamda
birbirine yakınlık ya da uzaklıkları bu sayılar üzerinden
ölçülebiliyordu. Bu tür bir temsilde bazı kelimeler sayısal olarak şu halde
olabiliyordu,</p>

<pre><code>cat:  (0.01359, ..., -0.2524, 1.0048, 0.06259)
mat:  (0.01396, ..., 0.033483, -0.10007, 0.1158)
chills: (-0.24776, ..., 0.079717, 0.23865, -0.014213)
sat:  (-0.35609, ..., -0.35413, 0.38511, -0.070976)
</code></pre>

<p>Derin YSA ile aynı sonuç kelime gömme (word embedding) mekanizması ile elde
ediliyor. Fikir aslında gayet basit ve dahiyane. Kelime yerine onları
temsil eden sayısal vektörler YSA'nın bir tabakasına "gömülür'' ve aynen
YSA'nın diğer katmanları gibi ağırlık olarak addedilip
eğitilirler. Başlangıçta tüm kelimelerin gömme vektörleri rasgele
sayılardır, eğitim ilerledikçe bu değerler anlamlı hale gelirler.</p>

<p>Kodlama kabaca şöyle: tüm dokümanlar üzerinden sözlüğü oluştururuz.
Kelimeler 1-hot vektörü değil, tek bir indis haline getirilir, üstteki
"cat'' sadece 300 sayısına dönüşür yani. Gömme tabakası için sadece
sınırlı sayıda kelimeyi alırız, mesela ilk 5'i, yani her dokümanın ilk 5
kelimesi tutulur, gerisi atılır, eğer eksik varsa dolgulama (padding) ile
sıfırlar eklenip 5'e getirilir. Tabii bu indis değerleri YSA için direk
kullanılamaz, bir sonraki aşama, YSA'ya bir gömme tabakası eklemek, YSA'nın
eğitimde kullanacağı esas değerler bunlar. Her (tek) kelimenin gömme boyutu
da önceden kararlaştırılır (gömme vektörünün sayısal boyutu), $n$ diyelim,
mesela $n=4$, eğer sözlük büyüklüğü $|V|$ ise, $n \times |V|$ boyutunda bir
büyük gömme referans matrisi elde edilir. </p>

<p><img src="nlp_02.png" alt="" /></p>

<p>Bu referans matrisin başlangıç değerleri rasgeledir. Bu örnekte YSA içinde
bulunan gömme girdi katmanının tamamı 5 x 4 = 20 olacaktır. Altta "cat
chills on a mat (kedi paspas üzerinde takılıyor)'' cümlesini görüyoruz,</p>

<p><img src="nlp_03.png" alt="" /></p>

<p>Üstteki girdiyi olduğu gibi alabilirdik, yani girdi katmanı 5 x 4
boyutundaki bir "tensor'' da olabilirdi (modern YSA araçları çok boyutlu
tensorlar ile rahatça çalışırlar), biz basitleştirme amacıyla vektörün
düzleştirildiğini düşünelim,</p>

<p><img src="nlp_04.png" alt="" /></p>

<p>Burada ilginç bir durum var, alışılagelen YSA kodlamasından farklı olarak
$x$ vektörüne "girdi'' dedik, fakat $x$'i bir tamamen bağlanmış ağırlık
tabakası olarak görmek daha doğru. Fakat bu ağırlık tabakası diğer ağırlık
tabakaları gibi de değil; Her eğitim veri noktasının içindeki kelimelerin
indisleri üzerinden <em>referans gömme matrisindeki</em> uygun satırlar
çekilip o anda bir $x$ haline getiriliyor. Ardından geriye yayılma ile YSA
hata düzeltme yapacağı zaman gömme referans matrisindeki uygun vektörler
güncelleniyor.</p>

<p>Bu kadar. Şimdi eğitim hedef değerlerine bakalım. Burada farklı yaklaşımlar
var, üstteki <em>Lineer Cebir</em> yazısında bahsedilen YSA komşu kelimeleri
tahmin etmeye uğraşıyordu. Bir başka numara bir cümleyi alıp içindeki tek
bir kelimeyi "bozmak'', oraya anlamsız bir kelime getirmek, ve bu yeni
cümleyi 0, bozulmamış olanını 1 etiketiyle eğitmek. Cümleler nasıl olsa hazır
var, onları bozmak kolay, bu şekilde iki kategorili bir sınıflama problemi
elde ediyoruz. Örnek boyutlarla [3],</p>

<p>$$
x \in \mathbb{R}^{20 \times 1}, 
W \in \mathbb{R}^{8 \times 20}, 
U \in \mathbb{R}^{8 \times 1}
$$ </p>

<p>olsa, örnek girdi,</p>

<p>$$
x = \left[\begin{array}{rrrrr}
x<em>{cat} &amp; x</em>{chills} &amp; x<em>{on} &amp; x</em>{a} &amp; x_{mat}
\end{array}\right]
$$</p>

<p>olacak şekilde, </p>

<p>$$ s = U^T a$$</p>

<p>$$ a = f(z)$$</p>

<p>$$ z = Wx + b$$</p>

<p>katmanları tasarlanabilir.</p>

<p><img src="nlp_05.png" alt="" /></p>

<p>TensorFlow </p>

<p>TF bağlamında gömme tabakası ile referans matrisi arasındaki ilişki
<code>embedding_lookup</code> çağrısı ile yapılıyor. Bu çağrı bir matriste indis
erişimi sadece,</p>

<div class="codehilite">
<pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>
<span class="n">tf</span><span class="o">.</span><span class="n">reset_default_graph</span><span class="p">()</span>

<span class="n">params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">30</span><span class="p">,</span><span class="mi">40</span><span class="p">])</span>
<span class="n">ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
     <span class="nb">print</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">params</span><span class="p">,</span><span class="n">ids</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</code></pre>
</div>

<pre><code>[10 20 30 40]
</code></pre>

<p>Fakat YSA içine konduğu zaman bu çağrı ve onun oluşturduğu katman geriye
yayılma sırasında nasıl davranacağını biliyor. </p>

<p>Şimdi daha geniş bir örnek olarak [2] verisi üzerinde üstte tarif edilen
türden basit ağ yapısını oluşturalım. Veri <em>Rotten Tomatoes</em> adlı film
yorum sitesinden alınan kullanıcı yorumları (yazdığı kelimeler yani) ve
kullanıcının filmi beğendi / beğenmedi şeklindeki hissiyatı 0/1 olarak
içeriyor (bozma işlemine gerek yok, her iki etiket için bol veri
mevcut). Bu bir doğal dil işleme ikisel sınıflama problemi. Veriye bakalım,</p>

<div class="codehilite">
<pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">data_helpers</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.contrib</span><span class="w"> </span><span class="kn">import</span> <span class="n">learn</span>

<span class="n">dev_sample_percentage</span> <span class="o">=</span> <span class="mf">.1</span>
<span class="n">positive_data_file</span> <span class="o">=</span> <span class="s2">&quot;./data/rt-polarity.pos&quot;</span>
<span class="n">negative_data_file</span> <span class="o">=</span> <span class="s2">&quot;./data/rt-polarity.neg&quot;</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">120</span> <span class="c1"># bir kelime icin gomme boyutu</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">200</span>

<span class="n">x_text</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data_helpers</span><span class="o">.</span><span class="n">load_data_and_labels</span><span class="p">(</span><span class="n">positive_data_file</span><span class="p">,</span> <span class="n">negative_data_file</span><span class="p">)</span>
</code></pre>
</div>

<div class="codehilite">
<pre><span></span><code><span class="nb">print</span> <span class="n">y</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">x_text</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="nb">print</span> <span class="n">y</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">x_text</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>
<span class="nb">print</span> <span class="n">y</span><span class="p">[</span><span class="mi">10000</span><span class="p">],</span> <span class="n">x_text</span><span class="p">[</span><span class="mi">10000</span><span class="p">]</span>
</code></pre>
</div>

<pre><code>[0 1] if you sometimes like to go to the movies to have fun , wasabi is a good place to start
[0 1] emerges as something rare , an issue movie that 's so honest and keenly observed that it does n't feel like one
[1 0] like mike is a slight and uninventive movie like the exalted michael jordan referred to in the title , many can aspire but none can equal
</code></pre>

<p>Örnek gösterdiğimiz üç yoruma bakıyoruz, birincisi, ikincisi pozitif,
üçüncüsü negatif. Yorumlarda kullanılan kelimelerde mesela ilkinde "a good
place to start (iyi bir başlangıç noktası)'' yorumu pozitifsel bir hava
taşıyor, üçüncüde "uninventive (bir yenilik yok)'' kelimesi kullanılmış,
negatif. YSA eğitildikten sonra bu kelimelerin doğru temsilsel (gömme)
ağırlıklarını ve onların 0/1 hedefine olan bağlantısını öğrenmeyi umuyoruz.</p>

<p>İndis matrisini yaratalım,</p>

<div class="codehilite">
<pre><span></span><code><span class="n">max_document_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x_text</span><span class="p">])</span>
<span class="nb">print</span> <span class="s1">&#39;dokuman buyuklugu&#39;</span><span class="p">,</span> <span class="n">max_document_length</span>
<span class="n">vocab_processor</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">VocabularyProcessor</span><span class="p">(</span><span class="n">max_document_length</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">vocab_processor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_text</span><span class="p">)))</span>
<span class="nb">print</span> <span class="n">x</span>
</code></pre>
</div>

<pre><code>dokuman buyuklugu 56
[[    1     2     3 ...,     0     0     0]
 [    1    31    32 ...,     0     0     0]
 [   57    58    59 ...,     0     0     0]
 ..., 
 [   75    84  1949 ...,     0     0     0]
 [    1  2191  2690 ...,     0     0     0]
 [11512     3   147 ...,     0     0     0]]
</code></pre>

<p>İndis için <code>x</code> kullanımı kafa karıştırmasın, [1] kodlaması o şekilde
seçmiş.</p>

<p>Ardından gömme yapılır, ve bu ağırlıklar <code>fully_connected</code> ile tam
bağlanmış YSA tabakasına verilir, oradan çıkan sonuç ise iki kategorili
softmax'e verilir. Tüm kod,</p>

<div class="codehilite">
<pre><span></span><code><span class="c1"># nlp1.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">data_helpers</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.contrib</span><span class="w"> </span><span class="kn">import</span> <span class="n">learn</span>

<span class="n">dev_sample_percentage</span> <span class="o">=</span> <span class="mf">.1</span>
<span class="n">positive_data_file</span> <span class="o">=</span> <span class="s2">&quot;./data/rt-polarity.pos&quot;</span>
<span class="n">negative_data_file</span> <span class="o">=</span> <span class="s2">&quot;./data/rt-polarity.neg&quot;</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">120</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">200</span>

<span class="n">x_text</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data_helpers</span><span class="o">.</span><span class="n">load_data_and_labels</span><span class="p">(</span><span class="n">positive_data_file</span><span class="p">,</span> <span class="n">negative_data_file</span><span class="p">)</span>

<span class="n">max_document_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x_text</span><span class="p">])</span>
<span class="n">vocab_processor</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">VocabularyProcessor</span><span class="p">(</span><span class="n">max_document_length</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">vocab_processor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_text</span><span class="p">)))</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">shuffle_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>
<span class="n">x_shuffled</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">shuffle_indices</span><span class="p">]</span>
<span class="n">y_shuffled</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">shuffle_indices</span><span class="p">]</span>

<span class="n">dev_sample_index</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="nb">int</span><span class="p">(</span><span class="n">dev_sample_percentage</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_dev</span> <span class="o">=</span> <span class="n">x_shuffled</span><span class="p">[:</span><span class="n">dev_sample_index</span><span class="p">],</span> <span class="n">x_shuffled</span><span class="p">[</span><span class="n">dev_sample_index</span><span class="p">:]</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_dev</span> <span class="o">=</span> <span class="n">y_shuffled</span><span class="p">[:</span><span class="n">dev_sample_index</span><span class="p">],</span> <span class="n">y_shuffled</span><span class="p">[</span><span class="n">dev_sample_index</span><span class="p">:]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary Size: </span><span class="si">{:d}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab_processor</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train/Dev split: </span><span class="si">{:d}</span><span class="s2">/</span><span class="si">{:d}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_dev</span><span class="p">)))</span>

<span class="n">tf</span><span class="o">.</span><span class="n">reset_default_graph</span><span class="p">()</span>

<span class="n">num_classes</span><span class="o">=</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">sequence_length</span><span class="o">=</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="n">input_x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">])</span>
<span class="n">input_y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">])</span>

<span class="c1"># rasgele agirliklar</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab_processor</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">),</span>
                                   <span class="n">embedding_dim</span><span class="p">],</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>

<span class="n">ec</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">input_x</span><span class="p">)</span>

<span class="c1"># duzlestir</span>
<span class="n">embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">ec</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">fully_connected</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">embed</span><span class="p">,</span> <span class="n">num_outputs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                                           <span class="n">activation_fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">losses</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">input_y</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span> 

<span class="n">correct_predictions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">input_y</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct_predictions</span><span class="p">,</span> <span class="s2">&quot;float&quot;</span><span class="p">))</span>

<span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">)</span>

<span class="n">grads_and_vars</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">compute_gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="n">train_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads_and_vars</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">global_step</span><span class="p">)</span>

<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>

<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

<span class="n">batches</span> <span class="o">=</span> <span class="n">data_helpers</span><span class="o">.</span><span class="n">batch_iter</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)),</span><span class="n">batch_size</span><span class="p">,</span><span class="n">num_epochs</span><span class="p">)</span>

<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables</span><span class="p">())</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batches</span><span class="p">):</span>

    <span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">)</span>    
    <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span> <span class="n">input_x</span><span class="p">:</span> <span class="n">x_batch</span><span class="p">,</span> <span class="n">input_y</span><span class="p">:</span> <span class="n">y_batch</span> <span class="p">}</span>    
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">30</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">feed_dict2</span> <span class="o">=</span> <span class="p">{</span> <span class="n">input_x</span><span class="p">:</span> <span class="n">x_dev</span><span class="p">,</span> <span class="n">input_y</span><span class="p">:</span> <span class="n">y_dev</span> <span class="p">}</span>
        <span class="n">train_acc</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">feed_dict2</span><span class="p">)</span>
        <span class="nb">print</span> <span class="n">train_acc</span><span class="p">,</span> <span class="n">test_acc</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">200</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># arada sirada modeli kaydet</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;/tmp/nlpembed1&quot;</span><span class="p">)</span>
</code></pre>
</div>

<p>Bu kodun başarısı yüzde 67 civarı. </p>

<p>Evrişim Tabakası</p>

<p>Fakat daha yapılacaklar var. NLP alanında DYSA moda olmadan önce mesela
lojistik regresyon ile eğitim yapıldığında bir n-gram yaklaşımı vardı. Bir
doküman sayısal hale getirilirken kelimeleri tek tek alabiliriz, ya da,
yanyana gelen her iki (2-gram), üç (3-gram) kelime demetlerini sanki başlı
başına kelimelermiş gibi sayısala çevirebiliriz. Eğer mesela "küçük ev''
ve "sarı kedi'' 2-gramları sürekli dökümanlarda beraber görülüyorsa, bu
kelime çiftinin bir sınıflayıcı kuvveti olabilir, ve n-gram yaklaşımı bunu
kullanmaya uğraşır. n-gram işlemi bu yaklaşımlarda çoğunlukla bir önişlem
(preprocessing) aşamasında veriden yeni veri çıkartarak yapılırdı
(dokümanın kelimelerini sırayla ikişer ikişer, üçer üçer okuyarak), DYSA
ile elimizde daha kullanışlı bir silah var. Evrişim.</p>

<p><img src="nlp_01.png" alt="" /></p>

<p>Üstte görülen örnekte gömme tabakası $9 \times 6$ boyutunda, bu tabaka
üzerinde farklı boyutlarda evrişim operasyonları uygulanıyor, mesela 2 x 6
boyutlu bir evrişim var (kırmızı renkli), üstten başlanıp birer birer aşağı
kaydırılarak ikinci tabakadaki sonuç elde ediliyor, aynı şekilde 3 x 6 bir
diğeri (sarı renkli), vs. Böylece ikinci seviyedeki vektörler elde
ediliyor, bu vektörler üzerine max-pool işlemi uygulanıyor üçüncü seviye
elde ediliyor, ve oradan gelen sonuçlar softmax'e veriliyor.</p>

<p>Evrişim bir blok olarak girdi üzerinde işletildiği için, yanyana olan
kelimeler arasında alaka bulabilmesi gayet normal. Ayrıca değişik
boyutlarda, pek çok farklı filtre tanımladık, yani pek çok farklı ilişkiyi
bu filtreler üzerinden yakalamaya uğraştık.</p>

<p>Evrişim içeren genişletilmiş kod altta.</p>

<div class="codehilite">
<pre><span></span><code><span class="c1"># nlp2.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">data_helpers</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.contrib</span><span class="w"> </span><span class="kn">import</span> <span class="n">learn</span>

<span class="n">dev_sample_percentage</span> <span class="o">=</span> <span class="mf">.1</span>
<span class="n">positive_data_file</span> <span class="o">=</span> <span class="s2">&quot;./data/rt-polarity.pos&quot;</span>
<span class="n">negative_data_file</span> <span class="o">=</span> <span class="s2">&quot;./data/rt-polarity.neg&quot;</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">filter_sizes</span> <span class="o">=</span> <span class="s2">&quot;3,4,5&quot;</span>
<span class="n">num_filters</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">dropout_keep_prob</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">l2_reg_lambda</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">70</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">200</span>

<span class="n">x_text</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data_helpers</span><span class="o">.</span><span class="n">load_data_and_labels</span><span class="p">(</span><span class="n">positive_data_file</span><span class="p">,</span> <span class="n">negative_data_file</span><span class="p">)</span>

<span class="n">max_document_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x_text</span><span class="p">])</span>
<span class="n">vocab_processor</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">VocabularyProcessor</span><span class="p">(</span><span class="n">max_document_length</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">vocab_processor</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_text</span><span class="p">)))</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">shuffle_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>
<span class="n">x_shuffled</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">shuffle_indices</span><span class="p">]</span>
<span class="n">y_shuffled</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">shuffle_indices</span><span class="p">]</span>

<span class="n">dev_sample_index</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="nb">int</span><span class="p">(</span><span class="n">dev_sample_percentage</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_dev</span> <span class="o">=</span> <span class="n">x_shuffled</span><span class="p">[:</span><span class="n">dev_sample_index</span><span class="p">],</span> <span class="n">x_shuffled</span><span class="p">[</span><span class="n">dev_sample_index</span><span class="p">:]</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_dev</span> <span class="o">=</span> <span class="n">y_shuffled</span><span class="p">[:</span><span class="n">dev_sample_index</span><span class="p">],</span> <span class="n">y_shuffled</span><span class="p">[</span><span class="n">dev_sample_index</span><span class="p">:]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary Size: </span><span class="si">{:d}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab_processor</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train/Dev split: </span><span class="si">{:d}</span><span class="s2">/</span><span class="si">{:d}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_dev</span><span class="p">)))</span>

<span class="n">tf</span><span class="o">.</span><span class="n">reset_default_graph</span><span class="p">()</span>

<span class="n">num_classes</span><span class="o">=</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">sequence_length</span><span class="o">=</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">filter_sizes</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">filter_sizes</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">)))</span>

<span class="n">input_x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">])</span>
<span class="n">input_y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">])</span>
<span class="n">dropout_keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">l2_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab_processor</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">),</span>
                                   <span class="n">embedding_dim</span><span class="p">],</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>

<span class="n">embedded_chars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">input_x</span><span class="p">)</span>
<span class="n">embedded_chars_expanded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">embedded_chars</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">pooled_outputs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">filter_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">filter_sizes</span><span class="p">):</span>
    <span class="n">filter_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">filter_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_filters</span><span class="p">]</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">(</span><span class="n">filter_shape</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">num_filters</span><span class="p">]))</span>
    <span class="n">conv</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
        <span class="n">embedded_chars_expanded</span><span class="p">,</span>
        <span class="n">W</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;VALID&quot;</span><span class="p">)</span>

    <span class="n">h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">conv</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>

    <span class="n">pooled</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">max_pool</span><span class="p">(</span>
        <span class="n">h</span><span class="p">,</span> <span class="n">ksize</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">sequence_length</span> <span class="o">-</span> <span class="n">filter_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pool&quot;</span><span class="p">)</span>
    <span class="n">pooled_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pooled</span><span class="p">)</span>

<span class="n">num_filters_total</span> <span class="o">=</span> <span class="n">num_filters</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">filter_sizes</span><span class="p">)</span>

<span class="n">h_pool</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">pooled_outputs</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">h_pool_flat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">h_pool</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_filters_total</span><span class="p">])</span>

<span class="n">h_drop</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">h_pool_flat</span><span class="p">,</span> <span class="n">dropout_keep_prob</span><span class="p">)</span>

<span class="n">l2_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

<span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">num_filters_total</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">]))</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">num_classes</span><span class="p">]))</span>

<span class="n">l2_loss</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_loss</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
<span class="n">l2_loss</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_loss</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">xw_plus_b</span><span class="p">(</span><span class="n">h_drop</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">losses</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">scores</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">input_y</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span> <span class="o">+</span> <span class="n">l2_reg_lambda</span> <span class="o">*</span> <span class="n">l2_loss</span>

<span class="n">correct_predictions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">input_y</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">correct_predictions</span><span class="p">,</span> <span class="s2">&quot;float&quot;</span><span class="p">))</span>

<span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">)</span>

<span class="n">grads_and_vars</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">compute_gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="n">train_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads_and_vars</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">global_step</span><span class="p">)</span>

<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>

<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

<span class="n">batches</span> <span class="o">=</span> <span class="n">data_helpers</span><span class="o">.</span><span class="n">batch_iter</span><span class="p">(</span>
    <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)),</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">)</span>

<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables</span><span class="p">())</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batches</span><span class="p">):</span>
    <span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">input_x</span><span class="p">:</span> <span class="n">x_batch</span><span class="p">,</span><span class="n">input_y</span><span class="p">:</span> <span class="n">y_batch</span><span class="p">,</span> <span class="n">dropout_keep_prob</span><span class="p">:</span> <span class="n">dropout_kp</span>
    <span class="p">}</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">30</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">feed_dict2</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">input_x</span><span class="p">:</span> <span class="n">x_dev</span><span class="p">,</span>
            <span class="n">input_y</span><span class="p">:</span> <span class="n">y_dev</span><span class="p">,</span>
            <span class="n">dropout_keep_prob</span><span class="p">:</span> <span class="n">dropout_kp</span>
        <span class="p">}</span>
        <span class="n">train_acc</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">feed_dict2</span><span class="p">)</span>
        <span class="nb">print</span> <span class="n">train_acc</span><span class="p">,</span> <span class="n">test_acc</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">100</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s2">&quot;/tmp/nlpembed2&quot;</span><span class="p">)</span>
</code></pre>
</div>

<p>Bu kod eğitim sonrası yüzde 74 başarıya erişti, kodun orijinal yazarı [1]
yüzde 76'yi görmüş, zaten bu veri seti üzerinde bilinen en iyi başarı bu
civarda.</p>

<p>Gömme Temsilinin Transferi</p>

<p>Ya kelimelerin komşuyla olan ilişkisini, ya da bozma tekniği, ya da başka
şekilde etiket elde edip eğittiğimiz gömülü referans matrisindeki
ağırlıkların anlamsal bir önemi olduğunu söylemiştik. O zaman, eğer
yeterince büyük bir sözlük, etiket ile eğitince elde ettiğimiz gömme
ağırlıklarının bir uygulamadan alınıp bir diğerinde de kullanılabilmesi
gerekir, değil mi? Bu sorunun cevabı evet. Gömme ağırlıklarını transfer
etmek hakikaten mümkün, ve bunu yapan pek çok kişi var.</p>

<p>Ödev</p>

<p>1'inci kodu işletin, ve gömme referans matris ağırlıklarına bakın. Örnek
bir kelime seçip o kelimenin gömme matrisindeki diğerlerine olan mesafesini
Öklitsel uzaklık hesabıyla bulun, en yakın on kelimeyi gösterin.</p>

<p>Kaynaklar</p>

<p>[1] Britz, <em>Implementing a CNN for Text Classification in TensorFlow</em>, <a href="http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/">http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/</a></p>

<p>[2] Cornell U., <em>Movie Review Data</em>, <a href="http://www.cs.cornell.edu/people/pabo/movie-review-data">http://www.cs.cornell.edu/people/pabo/movie-review-data</a></p>

<p>[3] Socher, <em>Deep Learning for NLP, without Magic), \url{https://nlp.stanford.edu/courses/NAACL2013/NAACL2013-Socher-Manning-DeepLearning.pdf</em></p>

<p>[4] Bayramlı, Lineer Cebir, <em>SVD ile Kümeleme, Benzerlik</em></p>

          <div id="container-d84f574876e65b2d8f0c7bae784c22b3"></div>

          <br/><a href="../index.html">Yukarı</a>
        </section>          
      </div>
    </body>
</html>
