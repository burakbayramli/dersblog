<p><a href="..">Yukarı</a></p>
<h1>Paralel Lojistik Regresyon, Eşle/İndirge</h1>
<!DOCTYPE html>
<html>
  <head>
    <title>Paralel Lojistik Regresyon, Eşle/İndirge
</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [["$","$"]]}
      });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    </script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1121997141902625"
                  crossorigin="anonymous"></script>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full">
</script>
</head>

<p>Lojistik regresyon kodunu eşle-indirge (map-reduce) üzerinden paralelize
etmek için literatüre [1-7] bakınca, genel yaklaşımın makinalara bölünen
veri parçaları üzerinde ayrı ayrı gradyan çıkışının (gradient aşçent)
işletilmesi ve sonuç $\theta$'ların son bir makinada ortalamasının alınması
olduğunu görürüz.</p>
<p>Daha önceki lojistik regresyon yazımızda iki farklı gradyan çıkış
algoritmasi görmüştük. Bu algoritmalardan kullanacağımız daha basit olanı,
her döngüde alpha'yı değiştiren versiyon değil tek alpha kullanan, ve kod
içinde zar atan değil, veriyi sırayla işleyen. Bunun birkaç sebebi var,
öncelikle altta göreceğimiz üzere veriyi Hadoop'a vermeden önce kendimiz
karıştıracağız, yani kod içinde zar atmaya gerek kalmayacak. İkincisi pek
çok makinada işlem yapıldığı için tek bir sabit üzerinden azaltma yapmak
mümkün değil (fakat her işleyicinin -değişmeyen- kendine has / ayrı bir
sabiti olabilir, bu konuyu ileride işleyebiliriz), bu sebeple ve basitlik
amacıyla tek sabitli kod kullanıldı. Ayrıca artık döngü (iterasyon) yok,
yani veri baştan sona bir kez tarandı mı, o makinanın işlemi bitecek. Fakat
büyük veri ortamında (ki zaten onun için Hadoop kullanıyoruz herhalde)
elimizde o kadar çok veri olacak ki bu verinin tamamını işleyince zaten
100,200 kere döngüyü işletmek ile aynı etkiyi almış oluyoruz.</p>
<p>Örnek veri olarak alttakini ürettik,</p>
<pre><code class="python">from pandas import *
mean1 = [10,10]
mean2 = [20,20]
cov = [[5,0],[0,5]]             
d1 = DataFrame(np.random.multivariate_normal(mean1,cov,10000))
d2 = DataFrame(np.random.multivariate_normal(mean2,cov,10000))
d1['labels'] = 1
d2['labels'] = 0
data = DataFrame(np.vstack((d1,d2)))
data.to_csv(&quot;testSet.txt&quot;,sep='\t',index=None,header=None)
print data[:4]
</code></pre>

<pre><code>           0          1  2
0  10.287025  11.158653  1
1   7.390719  12.214295  1
2  11.720941   8.711403  1
3  11.543380  11.627805  1
</code></pre>

<pre><code class="python">plt.plot(d1.ix[:,0],d1.ix[:,1],'b.')
plt.hold(True)
plt.plot(d2.ix[:,0],d2.ix[:,1],'r.') %
plt.hold(True)
plt.savefig('logreg1.png')
</code></pre>

<p><img alt="" src="logreg1.png" /></p>
<p>Altta veriyi işletmeden önce kendimiz karıştırıyoruz,</p>
<pre><code class="python">!sort --random-sort testSet.txt &gt; /tmp/testSet1.txt
</code></pre>

<pre><code class="python">'''
Logistic regression for map/reduce written for MRJob,
uses stochastic gradient descent.
'''
from mrjob.job import MRJob
from mrjob.protocol import PickleProtocol
import numpy as np
import os, thread

class MRLogisticRegression(MRJob):
    INTERNAL_PROTOCOL = PickleProtocol

    def __init__(self, *args, **kwargs):
        super(MRLogisticRegression, self).__init__(*args, **kwargs)
        self.n  = 1
        self.m = 3
        self.count = 0
        self.theta = np.ones((self.m,1))
        self.final_theta = np.zeros((self.m,self.n))

    def sigmoid(self, arr):
        return 1.0/(1+np.exp(-arr))

    def stoc_grad_ascent0(self, data_mat, label, theta):
        alpha = 0.01
        h = self.sigmoid(np.dot(data_mat,theta))
        theta = theta.T + (alpha * data_mat * (label - h))
        theta = theta.reshape((self.m,self.n))
        return theta

    def mapper(self, key, line):        
        tokens = map(np.float,line.split('\t'))
        data = np.append(1.0,np.array(tokens[:-1]))
        label = np.array(tokens[-1])
        self.theta = self.stoc_grad_ascent0(data, label, self.theta)

    def mapper_final(self):        
        yield (&quot;key1&quot;, self.theta)

    def reducer(self, key, tokens):
        for val in tokens:
            self.final_theta += val
            self.count += 1
        yield('result',str(self.final_theta / self.count))

if __name__ == '__main__':
    MRLogisticRegression.run()

</code></pre>

<p>Üstte eşleyici içinde tek bir tane anahtar üretiyoruz, tüm makinalarda tüm
eşleyiciler aynı anahtarı, bir kez üretiyor olacaklar. Bunun sebebi nedir?
Ne yapmaya çalıştığımızı hatırlayalım, tüm makinalarda lojistik regresyon
işletiyoruz, gradyan çıkışı yapıyoruz, ve sonuçta o makinanın işi bitince
elimizde tek bir tane ağırlık vektörü yani theta olacak. İlgilendiğimiz
sonuç bu, o yüzden çıktı stdout'a tek bir satır yazılıyor. Peki niye aynı
anahtar? Çünkü her makinadaki tüm ağırlık vektörlerinin "hep beraber" bir
noktada ortalamasının alınmasını istiyoruz, bunu Hadoop'a yaptırmanın bir
yolu herkese aynı anahtarı kullandırtmak, böylece bu anahtarlar tek bir
indirgeyiciye (ve makinaya) gidecek, ve orada ortalamaları alınacak. Tüm
eşleyicilerin sonucunun tek bir indirgeçiye gitmesi performans problemi
çıkartmaz mı? Çıkmaz, çünkü 1000 tane, 10000 tane eşleyici paralel iş
yapmış olabilir, ama işleri bitince elimizde 1000,10000 tane ağırlık
vektörü olacak, ve bu zaten tek makinanın rahatlıkla başa çıkabileceği bir
yüktür.</p>
<p>Bu yaklaşım, eşleyicinin her veri satırı başına bir ya da daha fazla
anahtar-değer satırı ürettiği yaklaşımdan (mesela klasik kelime sayma
problemi) biraz farklı, o sebeple bu farklılığı belirtmek istedik.</p>
<p>Bir püf nokta, her veri satırı için işletilen map'e de aslında anahtar
ürettirmiyoruz, tüm map çağrıları bittikten sonra son bir kez çağırılacak
map\_final'a bu işi yaptırıyoruz. Oraya gelinceye kadar (map içinde) değişen
theta'yı sürekli hafızada tutmuşuz, son noktaya gelince o sonucu aynı
anahtar ile eşleyerek üretiyoruz ve iş bitiyor.</p>
<p>Komut satırından işletelim:</p>
<pre><code class="python">!python logreg.py /tmp/testSet1.txt 
</code></pre>

<pre><code>using configs in /home/burak/.mrjob.conf
creating tmp directory /tmp/logreg.burak.20131201.234703.391390
writing to /tmp/logreg.burak.20131201.234703.391390/step-0-mapper_part-00000
Counters from step 1:
  (no counters found)
writing to /tmp/logreg.burak.20131201.234703.391390/step-0-mapper-sorted
&gt; sort /tmp/logreg.burak.20131201.234703.391390/step-0-mapper_part-00000
writing to /tmp/logreg.burak.20131201.234703.391390/step-0-reducer_part-00000
Counters from step 1:
  (no counters found)
Moving /tmp/logreg.burak.20131201.234703.391390/step-0-reducer_part-00000 -&gt; /tmp/logreg.burak.20131201.234703.391390/output/part-00000
Streaming final output from /tmp/logreg.burak.20131201.234703.391390/output
&quot;result&quot;    &quot;[[ 9.50705297]\n [-0.32580375]\n [-0.31237616]]&quot;
removing tmp directory /tmp/logreg.burak.20131201.234703.391390
</code></pre>

<pre><code class="python">def plot_theta(theta):
    x = np.array(arange(-10.0, 40.0, 0.1))
    y = np.array((-theta[0]-theta[1]*x)/theta[2])
    plot(x, y)
    hold(True)
    plot(d1.ix[:,0],d1.ix[:,1],'b.')
    hold(True)
    plot(d2.ix[:,0],d2.ix[:,1],'r.')
    hold(True)
    ylim(0,30)
    xlim(0,30)

theta = [9.50829527,-0.36317422,-0.34354905]
plot_theta(theta)
plt.savefig('logreg2.png')
</code></pre>

<p><img alt="" src="logreg2.png" /></p>
<p>Kaynaklar</p>
<p>[1] Smola, <em>Scalable Machine Learning, Optimization</em>, <a href="http://alex.smola.org/teaching/berkeley2012/slides/4_Optimization.pdf">http://alex.smola.org/teaching/berkeley2012/slides/4_Optimization.pdf</a></p>
<p>[2] Bhandarkar, <em>Modeling with Hadoop</em>, <a href="http://www.slideshare.net/hadoop/modeling-with-hadoop-kdd2011">http://www.slideshare.net/hadoop/modeling-with-hadoop-kdd2011</a></p>
<p>[3] Simianer, <em>Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative SMT</em>, <a href="http://simianer.de/P12-1002-slides.pdf">http://simianer.de/P12-1002-slides.pdf</a></p>
<p>[4] Allen, {\em A Python implementation of binary regularized logistic
  regression with stochastic gradient descent, packaged as scripts for use
  with Hadoop streaming}, <a href="https://github.com/elsevierlabs/logistic-regression-sgd-mapreduce">https://github.com/elsevierlabs/logistic-regression-sgd-mapreduce</a></p>