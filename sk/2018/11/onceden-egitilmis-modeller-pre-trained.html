<h1>Önceden Eğitilmiş Modeller (Pre-trained Models), Aktarımlı Öğrenme (Transfer Learning), Çiçek Tanıma</h1>
<p>Önceden Eğitilmiş Modeller (Pre-trained Models), Aktarımlı Öğrenme (Transfer Learning), Çiçek Tanıma</p>
<p>Çiçekleri resimlerinden tanımamızı sağlayacak bir yapay zeka programı yazmak istiyoruz. Fakat biliyoruz ki derin öğrenmede daha fazla veri daha iyidir, ama elde fazla veri yok. Veri alttaki gibi olabilir, bu veride 17 çeşit çiçeğin 80'er tane resmi var.</p>
<p>http://www.robots.ox.ac.uk/~vgg/data/flowers/17/</p>
<p>Ornek bazi resimler (ustteki iki dandofil, alttaki iki snowdrop)</p>
<p>Madem elde fazla veri yok, acaba başka bir amaç için eğitilmiş ama yine de obje / resim tanımak için hazırlanmış başka bir modeli alıp onun ağırlıklarını bir şekilde kullanamaz mıyız? Evet bu yapılabilir. ImageNet yarışmasında  bir resmi 1000 tane sınıfa atayabilen VGG16 modeli var mesela. ImageNet verisi milyonlarca resim icerir, her sinif basina yuzlerce resim mevcuttur. Bu veri uzerinde egitilmis VGG16'nin  bilinen bir evrişimsel (convolutional) derin yapısı var, yarışmada üst-5 sınıflamasında (gerçek etiket tahmin edilen en üst 5 etiketlerden biri mi) 92.5% elde etmiş, ve bu sonucu alan ağırlıklar biliniyor. </p>
<p>Güzel haber şu, Keras kütüphanesi bu tür ünlü YSA yapılarını ve ağırlıklarını paketine dahil etmiş. </p>
<p>from keras.applications.vgg16 import VGG16</p>
<p>base_model = VGG16(weights="imagenet")</p>
<p>base_model.summary()</p>
<p>diyerek VGG16'nin İmageNet ağırlıklarını alabiliriz  mesela. Üstteki kod şunu gösterir, </p>
<hr />
<p>Layer (type)                 Output Shape              Param #   </p>
<p>=================================================================</p>
<p>input_1 (InputLayer)         (None, 224, 224, 3)       0         </p>
<hr />
<p>block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      </p>
<hr />
<p>block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     </p>
<hr />
<p>block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         </p>
<hr />
<p>block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     </p>
<hr />
<p>block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    </p>
<hr />
<p>block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         </p>
<hr />
<p>block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    </p>
<hr />
<p>block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    </p>
<hr />
<p>block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    </p>
<hr />
<p>block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         </p>
<hr />
<p>block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   </p>
<hr />
<p>block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   </p>
<hr />
<p>block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   </p>
<hr />
<p>block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         </p>
<hr />
<p>block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   </p>
<hr />
<p>block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   </p>
<hr />
<p>block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   </p>
<hr />
<p>block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         </p>
<hr />
<p>flatten (Flatten)            (None, 25088)             0         </p>
<hr />
<p>fc1 (Dense)                  (None, 4096)              102764544 </p>
<hr />
<p>fc2 (Dense)                  (None, 4096)              16781312  </p>
<hr />
<p>predictions (Dense)          (None, 1000)              4097000   </p>
<p>=================================================================</p>
<p>Total params: 138,357,544</p>
<p>VGG16'nin yapısını bu çıktıda görüyoruz.</p>
<p>ImageNet ağırlıkları Internet'ten indirilip $HOME/.keras/models altına koyuluyor, ve bir sonraki çağrıda tekrar indirilmesi gerekmiyor. </p>
<p>Önceden eğitilmiş modeli kullanmak ise yarar çünkü her ne kadar VGG16 resimleri 1000 sınıfa ayirmak için yazılmış olsa da sonuçta pek çok katmanı var ve bu katmanlar sınıflamayı öğrenirken genel olarak "bir resim nasıl temsil edilir" işini de öğrenmiş oluyorlar. Ayrıca çok büyük bir veri seti üzerinde eğitildiği için bu işi iyi öğrenmiş olacağını farzediyoruz. O zaman mevcut modeli şu şekilde kullanabiliriz: 1. yöntem yeni eğitim verisindeki (çiçek resimleri) her noktayı teker teker VGG16'ya "tahmin" amaçlı veririz, ama çıktıyı en son katmandan değil sona yakın katmanlardan birinden alırız; bu katmandaki ağırlıklar bir resmi temsiliyet açısından oldukca iyi konumda olmadırlar (öyle ki bu ağırlıklar birkaç katman sonrasındaki tahminlerin yüzde 92 üstünde başarı elde etmesini sağlamışlar), bu ağırlıkları bizim yeni resmi temsil eden bir "özellik vektörü" olarak kullanırız (fikir alttaki bağlantıdan). </p>
<p>https://gogul09.github.io/software/flower-recognition-deep-learning</p>
<p>Bu vektörleri yeni eğitim verisindeki tüm resimler için kullanıp yeni bir eğitim / test verisi yaratırız, ve bu veri üzerinde artık Lojistik Regresyon bile kullanabiliriz. Öyle ya özellik vektörü düz bir veri haline geldi, artık basit regresyon iyi bir sonuç vermeli. Tabii basit regresyonun etiketleri ImageNet etiketlerinden farklı olacak, bizde 17 etiket var, ImageNet'te 1000. </p>
<p>Bahsettigimiz katman numarasi için Keras ile </p>
<p>base_model = VGG16(weights="imagenet")</p>
<p>model = Model(input=base_model.input, output=base_model.get_layer('fc1').output)</p>
<p>yeterli. Artik model.predict(..) dersek bir resim icin 4096 boyutlu özellik vektörü üretilir. </p>
<p>Tekniği çiçek resimlerinde deneyelim. Üstteki bağlantıdaki verinin zip halini kullanıyoruz, </p>
<p>https://drive.google.com/file/d/1N5VbewVJZjAhBPlFFavmG44HvZeE9Zs5/view?usp=sharing</p>
<p>Kod</p>
<p>from keras.applications.vgg16 import VGG16, preprocess_input</p>
<p>from keras.applications.xception import Xception, preprocess_input</p>
<p>from keras.preprocessing import image</p>
<p>from keras.models import Model</p>
<p>from keras.models import model_from_json</p>
<p>from keras.layers import Input</p>
<p>import re, collections, zipfile</p>
<p>import pandas as pd</p>
<p>import numpy as np</p>
<p>base_model = VGG16(weights="imagenet")</p>
<p>model = Model(input=base_model.input, output=base_model.get_layer('fc1').output)</p>
<p>with zipfile.ZipFile('[DIZIN]/17flowers.zip', 'r') as z:</p>
<p>im_files_orig = list(z.namelist())</p>
<p>im_files = [x for x in im_files_orig if ".jpg" in x and "image_" in x]</p>
<p>class_names = ["daffodil", "snowdrop", "lilyvalley", "bluebell", "crocus",</p>
<p>"iris", "tigerlily", "tulip", "fritillary", "sunflower", </p>
<p>"daisy", "coltsfoot", "dandelion", "cowslip", "buttercup",</p>
<p>"windflower", "pansy"]</p>
<p>features = []</p>
<p>labels   = []</p>
<p>with zipfile.ZipFile('[DIZIN]/17flowers.zip', 'r') as z:</p>
<p>for i,flower in enumerate(class_names):</p>
<p>label = class_names[i]</p>
<p>for f in im_files[(i<em>80):(i+1)</em>80]:</p>
<p>x = image.load_img(z.open(f), target_size=(224, 224))</p>
<p>x = image.img_to_array(x)</p>
<p>x = preprocess_input(np.expand_dims(x.copy(), axis=0))</p>
<p>feature = model.predict(x)</p>
<p>flat = feature.flatten()</p>
<p>features.append(flat)</p>
<p>labels.append(label)</p>
<p>labels_dict = {}</p>
<p>for x in labels:</p>
<p>if x not in labels_dict: labels_dict[x] = len(labels_dict)</p>
<p>labels2 = [labels_dict[x] for x in labels]</p>
<p>from sklearn.model_selection import train_test_split</p>
<p>x_train, x_test, y_train, y_test = train_test_split(features, labels2, random_state=42, test_size=0.05)</p>
<p>from sklearn.linear_model import LogisticRegression, SGDClassifier</p>
<p>clf = LogisticRegression()</p>
<p>clf.fit(x_train, y_train)</p>
<p>rank_1 = 0</p>
<p>rank_5 = 0</p>
<p>for (lab, feat) in zip(y_test, x_test):</p>
<p>predictions = clf.predict_proba(np.atleast_2d(feat))[0]</p>
<p>predictions = np.argsort(predictions)[::-1][:5]</p>
<p>if lab == predictions[0]:</p>
<p>rank_1 += 1</p>
<p>if lab in predictions:</p>
<p>rank_5 += 1</p>
<p>rank_1 = (rank_1 / float(len(y_test))) * 100</p>
<p>rank_5 = (rank_5 / float(len(y_test))) * 100</p>
<p>print (rank_1)</p>
<p>print (rank_5)</p>
<p>Bu teknikle elde edilecek sonuç tek tahmin üst-1 için yüzde 88, üst-5 için yüzde 100. Gayet iyi. Eğer eğitim için sadece eldeki verileri kullansaydık bu sonuç erişmek mümkün olmazdı. </p>
<ol>
<li>Yöntem</li>
</ol>
<p>Akla gelebilir, eğer fc1 katmanındaki ağırlıkları lojistik regresyon için kullandıysak, o katmandan dallanıp yeni bir derin ağ yaratıp bu ağın son katmanını 17 sınıf için tanımlasak ve bu ağı Keras üzerinden eğitsek olmaz mı? </p>
<p>Olur. Burada bazı numaralar, sadece son katmanların eğitilmesi için önceki katmanları "dondurmak", böylece o ağırlıklar hiç değişmez, sadece son katman değişir. Ya da üstte fc1'den özellik aldığımıza göre fc1'e kadar (dahil olmak üzere) dondururuz, sonra fc2 ve son etiket tahmin katmanını serbest bırakırız. Şuna benzer bir kod olabilir,</p>
<p>for i in range(len(base_model.layers-2)): ]
    model.layers[i].trainable = False</p>
<p>num_classes = 17</p>
<p>predictions = Dense(num_classes, activation = 'relu')(output = base_model.get_layer('fc2').output )</p>
<p>model = Model(input = base_model.input, output = predictions)</p>
<p>model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])</p>
<p>Tabii eğitim için veri balyası (batch) üreten bir kod eklenecek, bunlar üstteki eğitim şekline verilecek, vs. vs. Bu okuyucuya odev olsun.</p>
<p>Hangi yaklaşımın daha uygun olacağı uygulamaya göre değisebilir.</p>
<p>VGG-16 yerine imaj tanıma için eğitilmiş farklı bir YSA da kullanabilirdik. Mesela Keras'ta ünlü bir diğer model ResNet50 modelidir.</p>
<p>Soru</p>
<p>İmageNet 1000 kategorisinde neler var? 17 çiçek resmiyle alakası ne?</p>
<p>Cevap</p>
<p>1000 kategoride bir sürü günlük hayattan obje var, ev, köpek, kedi, araba, vs. Çiçek resmi de vardır herhalde, ama bizim 17 kategorideki çiçekler olmayabilir. Ama o 1000 kategori üzerinden milyonlara resme bakarak öğrenilmiş bir YSA, başlangıç noktası olarak 17 kategorili yeni çiçek resimleri için faydalı olur. </p>
<p><img alt="" src="image_0001.jpg" />
<img alt="" src="image_0002.jpg" />
<img alt="" src="image_0088.jpg" />
<img alt="" src="image_0087.jpg" />
<img alt="" src="vgg16.png" /></p>