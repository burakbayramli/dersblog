
<!DOCTYPE html>
<html>
  <head>
    <title></title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [["$","$"]]}
      });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    </script>
   <script type="text/javascript"
       src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full">
   </script>

   <link rel="stylesheet" type="text/css" media="screen" href="https://burakbayramli.github.io/css/style.css">
  </head>
    <body>
      <div id="header_wrap" class="outer">
        <header class="inner">
          <h1 id="project_title">
            <a href="https://burakbayramli.github.io" style="text-decoration:none; color:inherit;">dersblog</a>
          </h1>
          <h2 id="project_tagline"></h2>          
        </header>
      </div>
      <div id="main_content_wrap" class="outer">        
        <section id="main_content" class="inner">
        <h1></h1>
<h1>Otomatik Tercüme, Makine Tercümesi (Machine Translation)</h1>

<p>Dizin-Dizin İlişkisini Öğrenmek (Sequence to Sequence Learning)</p>

<p>Dillerarası otomatik tercüme yazılımı Google Translate ile popüler hale
geldi. Google bu servisi ilk kodladığında parça parça, istatistiki bir
yaklaşım kullanarak kodlamış, fakat 2017 yılında bu servis tamamen derin
öğrenme üzerinden işleyecek şekilde değiştirildi, kod satır sayısı
500,000'den 500'e indi!</p>

<p>DO bazlı tercüme sistemleri nasıl işler? </p>

<p><img src="seq_02.png" alt="" /></p>

<p>Not: kaynak cümlesi ters şekilde girilmiş, bu mimarı ilk teklif edildiğinde
bu şekilde yapılıyordu, fakat [3]'e göre her eğitim verisi için dinamik
şekilde yaratılabilen RNN hücreleri durumunda buna gerek yok.</p>

<p>Servisin temelinde, üstte görüldüğü gibi bir RNN tabakası var. Fakat bu RNN
yapısında ilk (soldaki) bölüm kodlayıcı, ikinci (sağdaki) bölüm kod çözücü
olarak planlanmış.  Eğitim verisinde kaynak ve hedef cümle beraber, yanyana
olarak hem girdi olarak veriliyor, ayrıca tercüme sonuç cümlesi alınıp bir
de etiket verisi olarak kullanılıyor, bir farkla, etiketteki cümle zaman
indisinde eğitimdeki sonuç cümlenin bir geri kaydırılmış hali.</p>

<p>Kaynak, sonuç tercüme cümleleri farklı boylarda olabilir, hem aynı eğitim
noktası içinde birbirlerinden, hem de değişik eğitim noktalarında
kendilerinden bile farklı boyutlarda olabilirler, bu sebeple RNN öğe
sayıları dinamik şekilde, her eğitim verisine göre farklı olacak. Fakat bu
farklı boyutlar karışıklık yaratmıyor, çünkü tercüme için önemli olan şey
kodlayıcı bloktan kod çözücü bloğa geçen gizli konum.</p>

<p>Bu konuma daha önceki yazılarda $h$ adı vermiştik. Eğitim süreci şöyle, tüm
cümleler + tüm kelimeler üzerinden bir sözlük oluştururuz, bu sözlüğe göre
her kelimeye bir tam sayı indis değeri atarız, sonra kelimeleri tam
sayılara çevirip gömme tabakasına veririz, bu tabaka reel sayı içeren
vektörlere dönüşür, ve eğitim ilerledikçe referans gömme matrisinde
kelimelerin temsil değerleri iyileşir. Bunlar otomatik oluyor tabii, biz
soldan sağa YSA'ya her eğitim veri noktasındaki kelimeleri teker teker
geçiyoruz, bir eğitim noktası için önce birinci kelimeyi ilk RNN öğesine,
oradan çıkan $h$'yi ve ikinci kelimeyi ikinci RNN öğesine, böyle devam
ediyor.</p>

<p>Kodlayıcıdan çıkış olduğu anda (dikkat hala tek bir eğitim noktasını
işliyoruz) elimizde olan $h$'nin özel bir anlamı var. Üstteki mimariye göre
bu gizli katman tüm kaynak cümleyi temsil eden bir $h$'dir. Başta pek
değildir ama zaman geçtikçe öyle olacaktır. $h$ boyutu önceden planlanan
şekilde, yani cümleye göre küçülüp büyüyen bir şey değil. Neyse tabii ileri
besleme orada durmuyor, kod çözücüye devam ediliyor, burada kelimeler sonuç
tercümeden geliyor, şimdi onun kelimelerini almaya başlıyoruz ve etikette
bahsettiğimiz şekilde kaydırılan kelimelere tekabül edecek şekilde eğitime
devam ediyoruz, ve sağa en sona gelince bir eğitim noktası ile işimiz
bitiyor.</p>

<p>Hedef kelimeleri softmax olarak planlanmış, yani kod çözücüdeki RNN öğeleri
mümkün tüm kelimeler üzerinden bir olasılık vektörü üretiyor. Gerçek dünya
uygulamalarında bu yaklaşım külfetli olabilir, çünkü sözlük çok büyük ise
softmax boyutu tek bir kelime çıktısı için olasılıkları temsil etmek için
çok fazla boyutlu olmalıdır, burada performansı iyileştirebilecek başka
bazı yaklaşımlar var, ama kavramsal olarak çıktının sanki her mümkün kelime
üzerinden bir softmax olduğunu düşünebiliriz.</p>

<p>Eğitim bittikten sonra hiç görülmemiş yeni test verisi için tercüme nasıl
yaparız? Biraz önce gördüğümüz gibi kaynak cümlenin kelimeleri soldan sağa
YSA'ya verilir, kod çözücüye geldiğimizde $h$ ile beraber Go sembolü
verilecektir, ve bu sembol sonuç tercümede ilk kelimeyi üretir. Tercümenin
ilk kelimesini bu şekilde elde etmiş oluruz. Eğer eğitim iyi yapılmışsa
derin YSA ilk kelimeyi güzel bir şekilde üretecektir (daha doğrusu softmax
tüm kelimelerin olasılıklarını hesaplar, biz bu olasılıklara göre en olası
kelimeyi örnekleme yaparak alırız). Sonra bu üretilen kelimeyi alıp alttan
YSA'ya (artık kod çözücüde tabii) beslemeye devam ederiz, mesela Go sonucu
"içeri'' kelimesi verilmiş, biz "içeri'' kelimesini alttan ikinci RNN
öğesine veririz, bu bize üstten "girmesine'' kelimesini üretebilir, böyle
devam ederiz.</p>

<p>Dikkat Etme Vektörü (Attention Vector)</p>

<p>Bazı yaklaşımlara göre kodlayıcı bloktan çıkan $h$ bir cümleyi temsil etmek
için yeterli görülmüyor, kod çözücü bloğundaki RNN öğelerinden kaynak
cümledeki tüm kelimelere giden bir dikkat etme vektörü üzerinden bağlantı
koyuluyor. Detaylar [2]'de bulunabilir. Alttaki örnekte İngilizce
"I am a student'', yani ben bir öğrenciyim cümlesinin Fransızca karşılığı
"Je suis etuidant'' gösterilmiş. </p>

<p><img src="attention.jpg" alt="" /></p>

<p>Ayrıca RNN katmanı tek bir zincir olmayabilir, üst üste konulmuş birkaç
katmandan da oluşuyor olabilir. Resimde istiflenmiş iki RNN seviyesi
görüyoruz mesela.</p>

<p>Not: Bilgisayar ile söyleşi yapılmasını sağlayan chatbot teknolojisi
aslında üstteki tercüme teknolojisinin değişik bir kullanımı sadece. Eğer
kaynak ve sonuç cümleler aynı cümlenin iki farklı dildeki karşılığı yerine
iki kişi arasındaki konuşmalar olsaydı, YSA yapısı gömme tabakası, cümleler
alakası üzerinden "bir konuşmayı'' öğrenmeye başlardı. "Nasılsın''
cümlesine "çok iyiyim'' karşılığı veriliyor, bu iki cümle ve onun gibi
cümleleri üstteki teknikle eğitince yavaş yavaş YSA nasıl karşılık
vereceğini öğrenebilmeye başlıyor. </p>

<p>Örnek kod [1] alttadır, veri [4]'ten.</p>

<div class="codehilite">
<pre><span></span><code><span class="c1"># translate.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pickle</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span><span class="o">,</span><span class="w"> </span><span class="nn">os</span>

<span class="n">checkpoint_path</span> <span class="o">=</span> <span class="s2">&quot;/tmp/model.ckpt&quot;</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">data_utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">process_data</span><span class="p">,</span><span class="n">split_data</span><span class="p">,</span><span class="n">generate_epoch</span><span class="p">,</span><span class="n">generate_batch</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">rnn_cell</span><span class="p">(</span><span class="n">FLAGS</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">scope</span><span class="p">):</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">):</span>
        <span class="n">rnn_cell_type</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">rnn_cell</span><span class="o">.</span><span class="n">BasicLSTMCell</span>
        <span class="n">single_cell</span> <span class="o">=</span> <span class="n">rnn_cell_type</span><span class="p">(</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">num_hidden_units</span><span class="p">)</span>
        <span class="n">single_cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">rnn_cell</span><span class="o">.</span><span class="n">DropoutWrapper</span><span class="p">(</span><span class="n">single_cell</span><span class="p">,</span>
            <span class="n">output_keep_prob</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="n">dropout</span><span class="p">)</span>
        <span class="n">stacked_cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">rnn_cell</span><span class="o">.</span><span class="n">MultiRNNCell</span><span class="p">(</span>
            <span class="p">[</span><span class="n">single_cell</span><span class="p">]</span> <span class="o">*</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">stacked_cell</span>

<span class="k">def</span><span class="w"> </span><span class="nf">rnn_inputs</span><span class="p">(</span><span class="n">FLAGS</span><span class="p">,</span> <span class="n">input_data</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">scope</span><span class="p">):</span>

    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">W_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;W_input&quot;</span><span class="p">,</span>
            <span class="p">[</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">num_hidden_units</span><span class="p">])</span>

    <span class="c1"># embeddings will be shape [input_data dimensions, num_hidden units]</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">W_input</span><span class="p">,</span> <span class="n">input_data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">embeddings</span>

<span class="k">def</span><span class="w"> </span><span class="nf">rnn_softmax</span><span class="p">(</span><span class="n">FLAGS</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">scope</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">W_softmax</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;W_softmax&quot;</span><span class="p">,</span>
            <span class="p">[</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">num_hidden_units</span><span class="p">,</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">sp_vocab_size</span><span class="p">])</span>
        <span class="n">b_softmax</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;b_softmax&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">sp_vocab_size</span><span class="p">])</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">W_softmax</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_softmax</span>
    <span class="k">return</span> <span class="n">logits</span>

<span class="k">class</span><span class="w"> </span><span class="nc">model</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">FLAGS</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder_inputs&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder_inputs&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">targets</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;targets&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">en_seq_lens</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">],</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;en_seq_lens&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sp_seq_lens</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">],</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;sp_seq_lens&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;encoder&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>

            <span class="c1"># Encoder RNN cell</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder_stacked_cell</span> <span class="o">=</span> <span class="n">rnn_cell</span><span class="p">(</span><span class="n">FLAGS</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
                <span class="n">scope</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span>

            <span class="c1"># Embed encoder inputs</span>
            <span class="n">W_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;W_input&quot;</span><span class="p">,</span>
                <span class="p">[</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">en_vocab_size</span><span class="p">,</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">num_hidden_units</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embedded_encoder_inputs</span> <span class="o">=</span> <span class="n">rnn_inputs</span><span class="p">(</span><span class="n">FLAGS</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">en_vocab_size</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span>

            <span class="c1"># Outputs from encoder RNN</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">all_encoder_outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dynamic_rnn</span><span class="p">(</span>
                <span class="n">cell</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_stacked_cell</span><span class="p">,</span>
                <span class="n">inputs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embedded_encoder_inputs</span><span class="p">,</span>
                <span class="n">sequence_length</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">en_seq_lens</span><span class="p">,</span> <span class="n">time_major</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s1">&#39;decoder&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>

            <span class="c1"># Initial state is last relevant state from encoder</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder_initial_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_state</span>

            <span class="c1"># Decoder RNN cell</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder_stacked_cell</span> <span class="o">=</span> <span class="n">rnn_cell</span><span class="p">(</span><span class="n">FLAGS</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
                <span class="n">scope</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span>

            <span class="c1"># Embed decoder RNN inputs</span>
            <span class="n">W_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;W_input&quot;</span><span class="p">,</span>
                <span class="p">[</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">sp_vocab_size</span><span class="p">,</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">num_hidden_units</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embedded_decoder_inputs</span> <span class="o">=</span> <span class="n">rnn_inputs</span><span class="p">(</span><span class="n">FLAGS</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_inputs</span><span class="p">,</span>
                <span class="n">FLAGS</span><span class="o">.</span><span class="n">sp_vocab_size</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span>

            <span class="c1"># Outputs from encoder RNN</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">all_decoder_outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dynamic_rnn</span><span class="p">(</span>
                <span class="n">cell</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder_stacked_cell</span><span class="p">,</span>
                <span class="n">inputs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embedded_decoder_inputs</span><span class="p">,</span>
                <span class="n">sequence_length</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sp_seq_lens</span><span class="p">,</span> <span class="n">time_major</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">initial_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder_initial_state</span><span class="p">)</span>

            <span class="c1"># Softmax on decoder RNN outputs</span>
            <span class="n">W_softmax</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;W_softmax&quot;</span><span class="p">,</span>
                <span class="p">[</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">num_hidden_units</span><span class="p">,</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">sp_vocab_size</span><span class="p">])</span>
            <span class="n">b_softmax</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;b_softmax&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">sp_vocab_size</span><span class="p">])</span>

            <span class="c1"># Logits</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder_outputs_flat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">all_decoder_outputs</span><span class="p">,</span>
                <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">num_hidden_units</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logits_flat</span> <span class="o">=</span> <span class="n">rnn_softmax</span><span class="p">(</span><span class="n">FLAGS</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_outputs_flat</span><span class="p">,</span>
                <span class="n">scope</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span>

            <span class="c1"># Loss with masking</span>
            <span class="n">targets_flat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">targets</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">losses_flat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy_with_logits</span><span class="p">(</span>
                <span class="n">logits</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">logits_flat</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">targets_flat</span>
            <span class="p">)</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">targets_flat</span><span class="p">))</span>
            <span class="n">masked_losses</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">*</span> <span class="n">losses_flat</span>
            <span class="n">masked_losses</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">masked_losses</span><span class="p">,</span>  <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">targets</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
                <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">masked_losses</span><span class="p">,</span> <span class="n">reduction_indices</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Optimization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">trainable_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">()</span>
        <span class="c1"># clip the gradient to avoid vanishing or blowing up gradients</span>
        <span class="n">grads</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_global_norm</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="n">trainable_vars</span><span class="p">),</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">max_gradient_norm</span><span class="p">)</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">trainable_vars</span><span class="p">))</span>

        <span class="c1">#self.saver = tf.train.Saver(tf.all_variables())</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sess</span><span class="p">,</span> <span class="n">FLAGS</span><span class="p">,</span> <span class="n">batch_encoder_inputs</span><span class="p">,</span> <span class="n">batch_decoder_inputs</span><span class="p">,</span>
        <span class="n">batch_targets</span><span class="p">,</span> <span class="n">batch_en_seq_lens</span><span class="p">,</span> <span class="n">batch_sp_seq_lens</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>

        <span class="n">input_feed</span> <span class="o">=</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_inputs</span><span class="p">:</span> <span class="n">batch_encoder_inputs</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder_inputs</span><span class="p">:</span> <span class="n">batch_decoder_inputs</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">targets</span><span class="p">:</span> <span class="n">batch_targets</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">en_seq_lens</span><span class="p">:</span> <span class="n">batch_en_seq_lens</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sp_seq_lens</span><span class="p">:</span> <span class="n">batch_sp_seq_lens</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">:</span> <span class="n">dropout</span><span class="p">}</span>
        <span class="n">output_feed</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_optimizer</span><span class="p">]</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">output_feed</span><span class="p">,</span> <span class="n">input_feed</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="k">class</span><span class="w"> </span><span class="nc">parameters</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_en_vocab_size</span> <span class="o">=</span> <span class="mi">30000</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_sp_vocab_size</span> <span class="o">=</span> <span class="mi">30000</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">100</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">20</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_units</span> <span class="o">=</span> <span class="mi">300</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-3</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate_decay_factor</span> <span class="o">=</span> <span class="mf">0.99</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_gradient_norm</span> <span class="o">=</span> <span class="mf">5.0</span>

<span class="k">def</span><span class="w"> </span><span class="nf">create_model</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">FLAGS</span><span class="p">):</span>
    <span class="n">tf_model</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">FLAGS</span><span class="p">)</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Created a new model&quot;</span><span class="p">)</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">initialize_all_variables</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">tf_model</span>

<span class="k">def</span><span class="w"> </span><span class="nf">restore_model</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">FLAGS</span><span class="p">):</span>
    <span class="n">tf_model</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">FLAGS</span><span class="p">)</span>
    <span class="n">tf_model</span><span class="o">.</span><span class="n">saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">tf_model</span>

<span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="n">FLAGS</span><span class="p">):</span>

    <span class="c1"># Load the data</span>
    <span class="n">en_token_ids</span><span class="p">,</span> <span class="n">en_seq_lens</span><span class="p">,</span> <span class="n">en_vocab_dict</span><span class="p">,</span> <span class="n">en_rev_vocab_dict</span> <span class="o">=</span> \
        <span class="n">process_data</span><span class="p">(</span><span class="s1">&#39;nmtdata/my_en.txt&#39;</span><span class="p">,</span> <span class="n">max_vocab_size</span><span class="o">=</span><span class="mi">30000</span><span class="p">,</span> <span class="n">target_lang</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">sp_token_ids</span><span class="p">,</span> <span class="n">sp_seq_lens</span><span class="p">,</span> <span class="n">sp_vocab_dict</span><span class="p">,</span> <span class="n">sp_rev_vocab_dict</span> <span class="o">=</span> \
        <span class="n">process_data</span><span class="p">(</span><span class="s1">&#39;nmtdata/my_sp.txt&#39;</span><span class="p">,</span> <span class="n">max_vocab_size</span><span class="o">=</span><span class="mi">30000</span><span class="p">,</span> <span class="n">target_lang</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Split into train and validation sets</span>
    <span class="n">train_encoder_inputs</span><span class="p">,</span> <span class="n">train_decoder_inputs</span><span class="p">,</span> <span class="n">train_targets</span><span class="p">,</span> \
        <span class="n">train_en_seq_lens</span><span class="p">,</span> <span class="n">train_sp_seq_len</span><span class="p">,</span> \
        <span class="n">valid_encoder_inputs</span><span class="p">,</span> <span class="n">valid_decoder_inputs</span><span class="p">,</span> <span class="n">valid_targets</span><span class="p">,</span> \
        <span class="n">valid_en_seq_lens</span><span class="p">,</span> <span class="n">valid_sp_seq_len</span> <span class="o">=</span> \
        <span class="n">split_data</span><span class="p">(</span><span class="n">en_token_ids</span><span class="p">,</span> <span class="n">sp_token_ids</span><span class="p">,</span> <span class="n">en_seq_lens</span><span class="p">,</span> <span class="n">sp_seq_lens</span><span class="p">,</span>
            <span class="n">train_ratio</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;nmtdata/vocab_en.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">en_vocab_dict</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="n">output</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;nmtdata/vocab_sp.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">sp_vocab_dict</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="n">output</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="c1"># Update parameters</span>
    <span class="n">FLAGS</span><span class="o">.</span><span class="n">en_vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">en_vocab_dict</span><span class="p">)</span>
    <span class="n">FLAGS</span><span class="o">.</span><span class="n">sp_vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sp_vocab_dict</span><span class="p">)</span>

    <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;len(en_vocab_dict)&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">en_vocab_dict</span><span class="p">))</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;len(sp_vocab_dict)&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sp_vocab_dict</span><span class="p">))</span>

    <span class="c1"># Start session</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Create new model or load old one</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">checkpoint_path</span> <span class="o">+</span> <span class="s2">&quot;.index&quot;</span>
        <span class="nb">print</span> <span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">restore_model</span><span class="p">(</span><span class="n">sess</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">FLAGS</span><span class="p">)</span>

        <span class="c1"># Training begins</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">epoch_num</span><span class="p">,</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">generate_epoch</span><span class="p">(</span><span class="n">train_encoder_inputs</span><span class="p">,</span>
            <span class="n">train_decoder_inputs</span><span class="p">,</span> <span class="n">train_targets</span><span class="p">,</span>
            <span class="n">train_en_seq_lens</span><span class="p">,</span> <span class="n">train_sp_seq_len</span><span class="p">,</span>
            <span class="n">FLAGS</span><span class="o">.</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)):</span>

            <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;EPOCH: </span><span class="si">%i</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch_num</span><span class="p">))</span>
            <span class="c1"># Decay learning rate</span>
            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> \
                <span class="p">(</span><span class="n">FLAGS</span><span class="o">.</span><span class="n">learning_rate_decay_factor</span> <span class="o">**</span> <span class="n">epoch_num</span><span class="p">)))</span>

            <span class="n">batch_loss</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="k">for</span> <span class="n">batch_num</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_encoder_inputs</span><span class="p">,</span> <span class="n">batch_decoder_inputs</span><span class="p">,</span>
                <span class="n">batch_targets</span><span class="p">,</span> <span class="n">batch_en_seq_lens</span><span class="p">,</span>
                <span class="n">batch_sp_seq_lens</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>

                <span class="n">loss</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">FLAGS</span><span class="p">,</span>
                    <span class="n">batch_encoder_inputs</span><span class="p">,</span> <span class="n">batch_decoder_inputs</span><span class="p">,</span> <span class="n">batch_targets</span><span class="p">,</span>
                    <span class="n">batch_en_seq_lens</span><span class="p">,</span> <span class="n">batch_sp_seq_lens</span><span class="p">,</span>
                    <span class="n">FLAGS</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>
                <span class="nb">print</span> <span class="p">(</span><span class="n">loss</span><span class="p">)</span>
                <span class="n">batch_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;mean: &#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">batch_loss</span><span class="p">))</span>

            <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Saving the model.&quot;</span><span class="p">)</span>
            <span class="n">model</span><span class="o">.</span><span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">FLAGS</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">()</span>
    <span class="n">train</span><span class="p">(</span><span class="n">FLAGS</span><span class="p">)</span>
</code></pre>
</div>

<p>Kaynaklar</p>

<p>[1] Mohandas, <em>The Neural Perspective, RNN - Part 3 - Encoder - Decoder</em>,<a href="https://theneuralperspective.com/2016/11/20/recurrent-neural-networks-rnn-part-3-encoder-decoder/">https://theneuralperspective.com/2016/11/20/recurrent-neural-networks-rnn-part-3-encoder-decoder/</a></p>

<p>[2] TensorFlow, <em>TensorFlow Neural Machine Translation Tutorial</em>, <a href="https://github.com/tensorflow/nmt">https://github.com/tensorflow/nmt</a></p>

<p>[3] Géron, <em>Hands-On Machine Learning with Scikit-Learn and TensorFlow</em></p>

<p>[4] ManyThings Verisi, İngilizce-Türkçe, <em>Tab-delimited Bilingual Sentence Pairs</em>, <a href="https://drive.google.com/uc?export=view&id=16fsAVPaPgp9gW9mdLmKz0OOR9lQ1M9WU">https://drive.google.com/uc?export=view&amp;id=16fsAVPaPgp9gW9mdLmKz0OOR9lQ1M9WU</a></p>


          <br/><a href="../index.html">Yukarı</a>
        </section>          
      </div>
    </body>
</html>
