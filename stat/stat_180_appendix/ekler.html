<h1>Ekler</h1>
<!DOCTYPE html>
<html>
  <head>
    <title>Ekler
</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [["$","$"]]}
      });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    </script>
<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full">
</script>
</head>

<p>Binom ve $p$ İçin Maksimum Olurluk Tahmini [1]</p>
<p>$$ L(p;x) = \prod_{i=1}^n f(x_i;p) = \prod_{i=1}^n {n \choose x} p^x(1-p)^{1-x} $$</p>
<p>Log alalım</p>
<p>$$ \log L(p;x) = 
\sum_{i=1}^n \log {n \choose x} + x \log p + (1-x) \log (1-p) $$</p>
<p>$p$'ye göre türevi alalım, bu sırada kombinasyon ifadesi ${n \choose x}$
içinde $p$ olmadığı için o yokolacaktır,</p>
<p>$$ \frac{\partial \log L(p)}{\partial p} =
\frac{x}{p} - \frac{n-x}{1-p}
$$</p>
<p>Maksimum değeri bulmak için sıfıra eşitleyelim ve $p$ için çözelim,</p>
<p>$$ 0 = \frac{x}{p} - \frac{n-x}{1-p} $$</p>
<p>$$  \frac{x}{p} = \frac{n-x}{1-p}  $$</p>
<p>$$ p(n-x)  = x(1-p) $$</p>
<p>$$ pn - px = x-px $$</p>
<p>$$ pn = x $$</p>
<p>$$ p = \frac{x}{n} $$</p>
<p>Yani $p$ için maksimum olurluk tahmini $x/n$. </p>
<p>Bernoulli dağılımı Binom dağılımına çok benzer, sadece onun baş kısmında
kombinasyon ifadesi yoktur. Fakat o ifade $p$'ye göre türevde nasıl olsa
yokolacağına göre Bernoulli dağılımı için de tahmin edici aynıdır.</p>
<hr>

<p>Bayes Usulü Güven Aralığı (Confidence Intervals) </p>
<p>Bayes ile bu hesabı yapmak için bir dağılımı baz almak lazım. Eğer sonuç
olarak bir tek sayı değil, bir dağılım elde edersek bu dağılım üzerinde
güvenlik hesaplarını yaparız. Mesela sonuç, sonsal dağılım (posterior) bir
Gaussian dağılım ise, bu dağılımın yüzde 95 ağırlığının nerede olduğu, ve
nasıl hesaplandığı bellidir.</p>
<p>Bayes Teorisi</p>
<p>$$ P(A \mid B)  = \frac{P(B \mid A)P(A)}{P(B)} $$</p>
<p>Veri analizi bağlamında diyelim ki deneyler yaparak tahmini olarak
hesaplamak (estimate) istediğimiz bir parametre var, bu bir protonun
kütlesi ya da bir ameliyat sonrası hayatta kalma oranı olabilir. Bu
durumlarda iki ayrı "olaydan" bahsetmemiz gerekir, B olayı spesifik bazı
ölçümlerin elde edilmesi "olayıdır", mesela ölçüm üç sayıdan oluşuyorsa,
biz bir ölçümde spesifik olarak ${0.2,4,5.4}$ değerlerini elde
etmişiz. İkinci olay bilmediğimiz parametrenin belli bir değere sahip
olması olacak. O zaman Bayes Teorisinin şu şekilde tekrar yazabiliriz, </p>
<p>$$ P(parametre \mid veri ) \propto P(veri \mid parametre)P(parametre) $$</p>
<p>$\propto$ işareti orantılı olmak (proportional to) anlamına geliyor. Böleni
attık çünkü o bir sabit (tamamen veriye bağlı, tahmini hesaplamak
istediğimiz parametreye bağlı değil). Tabii bu durumda sol ve sağ taraf
birbirine eşit olmaz, o yüzden eşitlik yerine orantılı olmak işaretini
kullandık. Bu çerçevede "belli bir sayısal sabit çerçevesinde birbirine
eşit (equal within a numeric constant)" gibi cümleler de görülebilir. </p>
<p>Örnek</p>
<p>Diyelim ki bir bozuk para ile 10 kere yazı-tura attık, ve sonuç altta</p>
<p>T H H H H T T H H H</p>
<p>Bu veriye bakarak paranın hileli olup olmadığını anlamaya
çalışacağız. Bayes ifadesini bu veriye göre yazalım,</p>
<p>$$ P(p | { \textrm{T H H H H T T H H H} } \propto 
P({ \textrm{T H H H H T T H H H} | p) P(p) }
$$</p>
<p>$P(p)$ ifadesi ne anlama gelir? Aslında bu ifadeyi $P([Dagilim] = p)$ olarak
görmek daha iyi, artık $p$ parametresini bir dağılımdan gelen bir özgün değer
olarak gördüğümüze göre, o dağılımın belli bir $p$'ye eşit olduğu zamanı
modelliyoruz burada. Her halükarda $P(p)$ dağılımını, yani onsel (prior)
olasılığı bilmiyoruz, hesaptan önce her değerin mümkün olduğunu biliyoruz, o
zaman bu onsel dağılımı düz (flat) olarak alırız, yani $P(p) = 1$.</p>
<p>$P({\textrm{T H H H H T T H H H} | p)$ ifadesi göz korkutucu olabilir, ama
buradaki her öğenin bağımsız özdeşçe dağılmış (independent identically
distributed) olduğunu görürsek, ama bu ifadeyi ayrı ayrı
$P({\textrm{T}|p)$ ve $P({\textrm{H}|p)$ çarpımları olarak
görebiliriz. $P({\textrm{T}|p) = p$ ve $P({\textrm{H}|p)=1-p$ olduğunu
biliyoruz. O zaman</p>
<p>$$ P(p | { \textrm{7 Tura, 3 Yazı} } \propto
p^7(1-p)^3
$$</p>
<p>Grafiklersek, </p>
<p><img alt="" src="stat_appendix_01.png" /></p>
<p>Böylece $p$ için bir sonsal dağılım elde ettik. Artık bu dağılımın yüzde 95
ağırlığının nerede olduğunu rahatça görebiliriz /
hesaplayabiliriz. Dağılımın tepe noktasının $p=0.7$ civarında olduğu
görülüyor. Bir dağılımla daha fazlasını yapmak ta mümkün, mesela bu
fonksiyonu $p$'ye bağlı başka bir fonksiyona karşı entegre etmek mümkün,
mesela beklentiyi bu şekilde hesaplayabiliriz.</p>
<p>Onsel dağılımın her noktaya eşit ağırlık veren birörnek (uniform) seçilmiş
olması, yani problemi çözmeye sıfır bilgiden başlamış olmamız, yöntemin bir
zayıflığı olarak görülmemeli. Yöntemin kuvveti elimizdeki bilgiyle başlayıp
onu net bir şekilde veri ve olurluk üzerinden sonsal tek dağılıma
götürebilmesi. Başlangıç ve sonuç arasındaki bağlantı gayet net. Fazlası da
var; ilgilendiğimiz alanı (domain) öğrendikçe, başta hiç bilmediğimiz onsel
dağılımı daha net, bilgili bir şekilde seçebiliriz ve bu sonsal dağılımı da
daha olması gereken modele daha yaklaştırabilir. </p>
<hr>

<p>Moment </p>
<p>Olasılık matematiğinde "moment üreten işlevler" olarak adlandırılan,
başlangıçta pek yararlı gibi gözükmesede bir takım matematiksel
özellikleri olduğu için, ispatlarda oldukça işe yarayan bir kavram
vardır.</p>
<p>Her rasgele değişkenin bir dağılımı olduğunu biliyoruz. Her rasgele
değişkenin de ayrıca bir moment üreten fonksiyonu da vardır. Ayrıca,
moment üreten fonksiyon ile rasgele değişken arasında bire-bir olarak
bir ilişki mevcuttur. "Bu neye yarar?" diye sorulabilir; Cevap olarak,
mesela cebirsel olarak türete türete bir moment'e geldiğimiz
düşünelim, ve tekrar başka bir taraftan, başka bir formülden gene
türete türete tekrar aynı moment işlevine geliyorsak, bu demektir ki,
iki taraftan gelen rasgele değişkenler (ve tekabül eden dağılımları)
birbirine eşittir. Bazı şartlarda moment üreten işlevler ile cebir
yapmak, dağılım fonksiyonlarından daha rahat olmaktadır.</p>
<p>Her rasgele değişken için, moment üreten işlev şöyle bulunur.</p>
<p>$X$ rasgele degiskenin moment ureten operasyonu</p>
<p>$M(t)=E(e^{tX})$ olarak gösterilir</p>
<p>Ayrıksal operasyonlar için</p>
<p>$$ M(t) = \sum_x e^{tx}p(x) $$</p>
<p>Sürekli işlevler için</p>
<p>$$ M(t) = \int_{-\infty}^{\infty} e^{tx}f(x) \mathrm{d} x   $$</p>
<p>Kuram</p>
<p>Gelelim yazımızın esas konusu olan kuramımıza.</p>
<p>Eğer $X_1, X_2...X_n$ bağımsız rasgele değişken ise, ve her değişkenin
$M_i(t)$ $i=1,2,3,...n$ olarak, öz olarak aynı olan birer moment üreten
işlevi var ise, o zaman,</p>
<p>$$ Y = \sum_{i=1}^n  aX_i $$</p>
<p>açılımı</p>
<p>$$ M_y(t) = \prod_{i=1}^n M(a_i t) $$</p>
<p>olacaktır. </p>
<p>İspat</p>
<p>$$ M_y(t) = E(e^{tY}=E(e^{t(a_1X_1+a_2X_2+..+a_nX_n)} $$</p>
<p>$$ = E[\exp(ta_1 X_1 ta_2X_2...+ta_nX_n)] $$</p>
<p>$$ = E[\exp(ta_1X_1)+\exp(ta_2X_2)+ ... + \exp(ta_nX_n)] $$</p>
<p>$$ = E[\exp(ta_1X_1)]+E[\exp(ta_2X_2)]+ ... + E[\exp(ta_nX_n)]$$</p>
<p>Daha önce belirttiğimiz gibi</p>
<p>$$ M_i(t) = E[\exp(tX_i)] $$</p>
<p>olduğuna göre ve $t$ yerine $ta_i$ koyulduğunu düşünelim</p>
<p>$$ M_y(t) = \prod_{i=1}^n M_y(a_it) $$</p>
<p>olacaktır. </p>
<p>Bunu $M_y(t)= (M_i(a_it))^n$ şeklinde de gösterebiliriz. </p>
<hr>

<p>Markov'un Eşitsizliği (Markov's Inequality)</p>
<p>$X$ bir negatif olmayan rasgele değişken olsun ve farz edelim ki $E(X)$
mevcut [1]. O zaman her $t &gt; 0$ için</p>
<p>$$ P(X&gt;t) \le \frac{E(X)}{t}$$</p>
<p>doğru olmalıdır. </p>
<p>İspat</p>
<p>$X &gt; 0$ olduğuna göre, </p>
<p>$$ 
E(X) 
= \int _{0}^{\infty} x f(x) \mathrm{d} x 
= \int _{0}^{t} x f(x) \mathrm{d} x + \int _{t}^{\infty} x f(x) \mathrm{d} x =
$$</p>
<p>$$ 
\ge \int _{t}^{\infty} x f(x) \mathrm{d} x \ge t \int _{t}^{\infty} f(x) \mathrm{d} x
= t P(X &gt; t)
$$</p>
<p>Çebişev Eşitsizliği (Chebyshev's Inequality)</p>
<p>Herhangi bir $t$ değeri için, </p>
<p>$$ P(|X-\mu| &gt; t) \le \frac{\sigma^2}{t^2} $$</p>
<p>ve </p>
<p>$$ P(|Z| \ge k) \le \frac{1}{k^2}$$</p>
<p>ki $Z = (X-\mu)/\sigma$, ve $E(X) = \mu$. Bunun bazı akılda kalabilecek
ilginç sonuçları $P(|Z| &gt; 2) &lt; 1/4$ ve $P(|Z| &gt; 3) &lt; 1/9$ olabilir.</p>
<p>İspat</p>
<ol>
<li>Yöntem</li>
</ol>
<p>Üstteki Markov'un eşitsizliğini kullanırız, oradan şu sonuca varırız, </p>
<p>$$ 
P(|X-\mu| \ge t) = P(|X-\mu|^2 \ge t^2 ) \le \frac{E(X-\mu)^2}{t^2} 
= \frac{\sigma^2}{t^2}<br />
$$</p>
<p>İkinci kısım $t=k\sigma$ kullanılarak elde edilebilir.</p>
<ol>
<li>Yöntem</li>
</ol>
<p>Olasılık matematiğinde, büyük sayılar kuramı adında anılan ve olasılık
matematiğinin belkemiğini oluşturan kuramı ispatlamak için, diğer bir kuram
olan Çebişev eşitsizliğini de anlamamız gerekiyor. Çebişev eşitsizliği bir
rasgele değişken, onun ortalaması (beklentisi) ve herhangi bir sabit sayı
arasındaki üçlü arasında bir 'eşitsizlik' bağlantısı kurar, ve bu bağlantı
diğer olasılık işlemlerimizde ispat verisi olarak işimize yarar.</p>
<p>İspata başlayalım. Entegral ile olasılık hesabı yapmak için bize bir $x$
uzayı lazım.</p>
<p>$$ \mathbb{R} = {x: |x-\mu| &gt; t} $$</p>
<p>Yani $\mathbb{R}$ uzayı, $x$ ile ortalamasının farkının, $t$'den büyük olduğu bütün
sayıların kümesidir.</p>
<p>O zaman, </p>
<p>$$ P(|X-\mu| &gt; t) = \int_R f(x) \mathrm{d} x $$</p>
<p>Dikkat edelim $P(..)$ içindeki formül, küme tanımı ile aynı. O yüzden $P()$
hesabı ortada daha olmayan, ama varolduğu kesin bir dağılım fonksiyonu
tanımlamış da oluyor. Buna $f(x)$ deriz. $P()$'in, $f(x)$ fonksiyonunun $R$
üzerinden entegral olduğunu olasılığa giriş dersinden bilmemiz lazım. </p>
<p>Eger $x \in R$ dersek o zaman</p>
<p>$$ \frac{|x-\mu|^2}{t^2} \ge 1 $$</p>
<p>t'nin denkleme bu şekilde nereden geldiği şaşkınlık yaratabilir. Daha önce
tanımlanan şu ibareye dikkat edelim, $x: |x-u| &gt; t$ diye belirtmiştik. Bu
ifadeyi değiştirerek, yukarıdaki denkleme gelebiliriz.</p>
<p>Devam edersek, elimizdeki 1'den büyük bir değer var. Bu değeri kullanarak,
aşağıdaki tanımı yapmamız doğru olacaktır.</p>
<p>$$
\int_R f(x) \mathrm{d} x \le \int_R \frac{(x-\mu)^2}{t^2}f(x) \mathrm{d} x \le
\int_{-\infty}^{\infty}\frac{(x-\mu)^2}{t^2}f(x) \mathrm{d} x 
$$</p>
<p>Ortadaki entegral niye birinci entegralden büyük? Çünkü orta entegraldeki
$f(x)dx$ ibaresinden önce gelen kısmın, her zaman 1'den büyük olacağını
belirttiğimize göre, ikinci entegralin birinciden büyük olması normaldir,
çünkü birinci entegral $f(x)$ olasılık dağılımına bağlı, entegral ise bir
alan hesabıdır ve olasılık dağılımlarının sonsuzlar arasındaki entegrali
her zaman 1 çıkar, kaldı ki üstteki $x$'in uzayını daha da daralttık.</p>
<p>Evet...Üçüncü entegral ispata oldukça yaklaştı aslında. Standart sapma
işaretini hala ortada göremiyoruz, fakat son entegraldeki ibare standart
sapma değerini zaten içeriyor. Önce daha önceki olasılık natematiği
bilgimize dayanarak, standart sapmanın tanımını yazıyoruz. Dikkat edelim,
bu ibare şu anki ispatımız dahilinden değil, haricinden önceki bilgimize
dayanarak geldi. Standart sapmanın tanımı şöyledir.</p>
<p>$$ \sigma^2 = \int_{-\infty}^{\infty} (x-\mu)^2f(x) \mathrm{d} x $$</p>
<p>O zaman</p>
<p>$$
\frac{\sigma^2}{t^2}
= \int_{-\infty}^{\infty}\frac{(x-\mu)^2}{t^2}f(x)\mathrm{d} x
$$</p>
<p>yani</p>
<p>$$
\int_R f(x) \mathrm{d} x \le \frac{\sigma^2}{t^2} = 
\int_{-\infty}^{\infty} \frac{(x-\mu)^2}{t^2}f(x) \mathrm{d} x
$$</p>
<p>ki $\int_R f(x) \mathrm{d} x$ zaten $P(|X-\mu| &gt; t)$ olarak tanımlanmıştı. </p>
<p>Örnek</p>
<p>Diyelim ki bir tahmin edicimiz var, onu test etmek istiyoruz, bu bir yapay
sinir ağı (YSA) olabilir, ve elimizde $n$ tane test verisi var. Eğer tahmin
edici, yani YSA, hatalı ise $X_i=1$ olsun, haklı ise $X_i=0$ olsun. O zaman
gözlenen hata oranı (observed error rate)
$\overline{X}_n = n^{-1}\sum _{i=1}^{n} X_i$ olacaktır. Rasgele değişken
çıktılarına bakarak bunu bir $p$'si bilinmeyen bir Bernoulli dağılımından
geliyormuş gibi kabul edebileceğimizi görebiliriz. İstediğimiz gerçek -ama
bilinmeyen- $p$ hakkında irdeleme yapmak. $\overline{X}_n$'in gerçek
$p$'nin $\epsilon$ yakınında olmama olasılığı nedir?</p>
<p>Bernoulli'lerin özelliklerinden biliyoruz ki </p>
<p>$$ V(\overline{X}_n) = V(X_1) / n = p(1-p)/n$$</p>
<p>Çebişev uygulayınca, </p>
<p>$$ 
P(|\overline{X}_n - p| &gt; \epsilon) \le \frac{V(\overline{X}_n)}{\epsilon^2}
= \frac{p(1-p)}{n\epsilon^2} \le \frac{1}{4n\epsilon^2}
$$</p>
<p>Hatırlarsak Bernoulli için $E(X)=p$. Son geçiş mümkün oldu çünkü her $p$
için $p(1-p) \le \frac{1}{4}$ olmak zorundadır. Öyle değil mi? $p(1-p)$'nin
alabileceği en büyük değer $p=1/2$ içindir, bundan farklı her $p$ değeri
$1/4$'ten küçük bir çarpım verir, mesela $p=1/3$ için
$1/3 \cdot 2/3 = 2/9$.</p>
<p>O zaman, ve diyelim ki $\epsilon = .2$ ve $n=100$ için $0.0625$ sınırını
elde ederiz. </p>
<p>Hoeffding'in Eşitsizliği</p>
<p>Bu eşitsizlik Markov'un eşitsizliğine benziyor, ama daha keskin sonuçlar
verebiliyor, yani ufak güven aralıkları elde edebiliyoruz, ki bu daha fazla
kesinlik demektir. Bu eşitsizliği iki bölüm olarak vereceğiz, </p>
<p>$Y_1,Y_2,..,Y_n$ bağımsız gözlemler olsunlar, ki $E(Y_i)=0$ ve
$a_i \le Y_i \le b_i$ doğru olacak şekilde. O zaman herhangi bir $t&gt;0$ için </p>
<ol>
<li>Teori</li>
</ol>
<p>$$ 
P \bigg( 
\sum _{i=1}^{n} Y_i \ge \epsilon \le e^{-t\epsilon} 
\prod _{i=1}^{n} e^{{t^2}(b_i-a_i)^2 / 8}
\bigg)
$$</p>
<ol>
<li>Teori</li>
</ol>
<p>$X_1,..,X_n \sim Bernoulli(p)$ olsun. O zaman herhangi bir $\epsilon &gt; 0$ icin</p>
<p>$$ P(|\overline{X}_n -p| &gt; \epsilon ) \le 2e^{-2n\epsilon^2}$$</p>
<p>doğru olmalıdır ki, daha önce gördüğümüz gibi,
$\overline{X}_n = n^{-1}\sum _{i=1}^{n} X_i$ olacak şekilde. </p>
<p>İspat için bkz [1, sf. 67]. </p>
<p>Örnek</p>
<p>Diyelim ki $X_1,..,X_n \sim Bernoulli(p)$. $n=100$ ve $\epsilon=.2$
olsun. Çebişev esitsizligi ile </p>
<p>$$ P(|\overline{X}_n - p| &gt; \epsilon ) \le 0.0625 $$</p>
<p>elde etmiştik. Hoeffding'e göre</p>
<p>$$ 
P(|\overline{X}_n - p| &gt; \epsilon ) \le 2e^{-2 (100)(.2)^2} = 0.00067
$$</p>
<p>elde ederiz, ki bu Cebisev'den gelen $0.0625$'e göre çok daha ufak bir
değerdir.</p>
<p>Kaynaklar</p>
<p>[1] Wasserman, <em>All of Statistics</em></p>
<hr>

<p>z-Tablosu</p>
<p>Nasıl okunur? Z-değeri -0.8994 için z kolonundan aşağı inilir, ve -0.8
bulunur, x.x9xx yani 9 için .09 kolonuna gidilir ve bu kesişmedeki değer
okunur, .1867, yuvarlanarak .19 da kabul edilebilir. </p>
<p><img alt="" src="stat_appendix_02.png" /></p>
<p>\ \ z \ \ \  .00  \ \ \ \ \ .01 \ \ \ \ \ .02 \ \ \ \ \ .03 \ \ \ \ \ .04 \
\ \ \ .05 \ \ \ .06 \
\ \ .07 \ \ \ .08 \ \ \ .09</p>
<p>-3.4 .0003 .0003 .0003 .0003 .0003 .0003 .0003 .0003 .0003 .0002</p>
<p>-3.3 .0005 .0005 .0005 .0004 .0004 .0004 .0004 .0004 .0004 .0003</p>
<p>-3.2 .0007 .0007 .0006 .0006 .0006 .0006 .0006 .0005 .0005 .0005</p>
<p>-3.1 .0010 .0009 .0009 .0009 .0008 .0008 .0008 .0008 .0007 .0007</p>
<p>-3.0 .0013 .0013 .0013 .0012 .0012 .0011 .0011 .0011 .0010 .0010</p>
<p>-2.9 .0019 .0018 .0018 .0017 .0016 .0016 .0015 .0015 .0014 .0014</p>
<p>-2.8 .0026 .0025 .0024 .0023 .0023 .0022 .0021 .0021 .0020 .0019</p>
<p>-2.7 .0035 .0034 .0033 .0032 .0031 .0030 .0029 .0028 .0027 .0026</p>
<p>-2.6 .0047 .0045 .0044 .0043 .0041 .0040 .0039 .0038 .0037 .0036</p>
<p>-2.5 .0062 .0060 .0059 .0057 .0055 .0054 .0052 .0051 .0049 .0048</p>
<p>-2.4 .0082 .0080 .0078 .0075 .0073 .0071 .0069 .0068 .0066 .0064</p>
<p>-2.3 .0107 .0104 .0102 .0099 .0096 .0094 .0091 .0089 .0087 .0084</p>
<p>-2.2 .0139 .0136 .0132 .0129 .0125 .0122 .0119 .0116 .0113 .0110</p>
<p>-2.1 .0179 .0174 .0170 .0166 .0162 .0158 .0154 .0150 .0146 .0143</p>
<p>-2.0 .0228 .0222 .0217 .0212 .0207 .0202 .0197 .0192 .0188 .0183</p>
<p>-1.9 .0287 .0281 .0274 .0268 .0262 .0256 .0250 .0244 .0239 .0233</p>
<p>-1.8 .0359 .0351 .0344 .0336 .0329 .0322 .0314 .0307 .0301 .0294</p>
<p>-1.7 .0446 .0436 .0427 .0418 .0409 .0401 .0392 .0384 .0375 .0367</p>
<p>-1.6 .0548 .0537 .0526 .0516 .0505 .0495 .0485 .0475 .0465 .0455</p>
<p>-1.5 .0668 .0655 .0643 .0630 .0618 .0606 .0594 .0582 .0571 .0559</p>
<p>-1.4 .0808 .0793 .0778 .0764 .0749 .0735 .0721 .0708 .0694 .0681</p>
<p>-1.3 .0968 .0951 .0934 .0918 .0901 .0885 .0869 .0853 .0838 .0823</p>
<p>-1.2 .1151 .1131 .1112 .1093 .1075 .1056 .1038 .1020 .1003 .0985</p>
<p>-1.1 .1357 .1335 .1314 .1292 .1271 .1251 .1230 .1210 .1190 .1170</p>
<p>-1.0 .1587 .1562 .1539 .1515 .1492 .1469 .1446 .1423 .1401 .1379</p>
<p>-0.9 .1841 .1814 .1788 .1762 .1736 .1711 .1685 .1660 .1635 .1611</p>
<p>-0.8 .2119 .2090 .2061 .2033 .2005 .1977 .1949 .1922 .1894 .1867</p>
<p>-0.7 .2420 .2389 .2358 .2327 .2296 .2266 .2236 .2206 .2177 .2148</p>
<p>-0.6 .2743 .2709 .2676 .2643 .2611 .2578 .2546 .2514 .2483 .2451</p>
<p>-0.5 .3085 .3050 .3015 .2981 .2946 .2912 .2877 .2843 .2810 .2776</p>
<p>-0.4 .3446 .3409 .3372 .3336 .3300 .3264 .3228 .3192 .3156 .3121</p>
<p>-0.3 .3821 .3783 .3745 .3707 .3669 .3632 .3594 .3557 .3520 .3483</p>
<p>-0.2 .4207 .4168 .4129 .4090 .4052 .4013 .3974 .3936 .3897 .3859</p>
<p>-0.1 .4602 .4562 .4522 .4483 .4443 .4404 .4364 .4325 .4286 .4247</p>
<p>0.0 .5000 .4960 .4920 .4880 .4840 .4801 .4761 .4721 .4681 .4641</p>
<hr>

<p>\ \ z \ \ \  .00  \ \ \ \ \ .01 \ \ \ \ \ .02 \ \ \ \ \ .03 \ \ \ \ \ .04 \
\ \ \ .05 \ \ \ .06 \
\ \ .07 \ \ \ .08 \ \ \ .09</p>
<p>0.0 .5000 .5040 .5080 .5120 .5160 .5199 .5239 .5279 .5319 .5359</p>
<p>0.1 .5398 .5438 .5478 .5517 .5557 .5596 .5636 .5675 .5714 .5753</p>
<p>0.2 .5793 .5832 .5871 .5910 .5948 .5987 .6026 .6064 .6103 .6141</p>
<p>0.3 .6179 .6217 .6255 .6293 .6331 .6368 .6406 .6443 .6480 .6517</p>
<p>0.4 .6554 .6591 .6628 .6664 .6700 .6736 .6772 .6808 .6844 .6879</p>
<p>0.5 .6915 .6950 .6985 .7019 .7054 .7088 .7123 .7157 .7190 .7224</p>
<p>0.6 .7257 .7291 .7324 .7357 .7389 .7422 .7454 .7486 .7517 .7549</p>
<p>0.7 .7580 .7611 .7642 .7673 .7704 .7734 .7764 .7794 .7823 .7852</p>
<p>0.8 .7881 .7910 .7939 .7967 .7995 .8023 .8051 .8078 .8106 .8133</p>
<p>0.9 .8159 .8186 .8212 .8238 .8264 .8289 .8315 .8340 .8365 .8389</p>
<p>1.0 .8413 .8438 .8461 .8485 .8508 .8531 .8554 .8577 .8599 .8621</p>
<p>1.1 .8643 .8665 .8686 .8708 .8729 .8749 .8770 .8790 .8810 .8830</p>
<p>1.2 .8849 .8869 .8888 .8907 .8925 .8944 .8962 .8980 .8997 .9015</p>
<p>1.3 .9032 .9049 .9066 .9082 .9099 .9115 .9131 .9147 .9162 .9177</p>
<p>1.4 .9192 .9207 .9222 .9236 .9251 .9265 .9279 .9292 .9306 .9319</p>
<p>1.5 .9332 .9345 .9357 .9370 .9382 .9394 .9406 .9418 .9429 .9441</p>
<p>1.6 .9452 .9463 .9474 .9484 .9495 .9505 .9515 .9525 .9535 .9545</p>
<p>1.7 .9554 .9564 .9573 .9582 .9591 .9599 .9608 .9616 .9625 .9633</p>
<p>1.8 .9641 .9649 .9656 .9664 .9671 .9678 .9686 .9693 .9699 .9706</p>
<p>1.9 .9713 .9719 .9726 .9732 .9738 .9744 .9750 .9756 .9761 .9767</p>
<p>2.0 .9772 .9778 .9783 .9788 .9793 .9798 .9803 .9808 .9812 .9817</p>
<p>2.1 .9821 .9826 .9830 .9834 .9838 .9842 .9846 .9850 .9854 .9857</p>
<p>2.2 .9861 .9864 .9868 .9871 .9875 .9878 .9881 .9884 .9887 .9890</p>
<p>2.3 .9893 .9896 .9898 .9901 .9904 .9906 .9909 .9911 .9913 .9916</p>
<p>2.4 .9918 .9920 .9922 .9925 .9927 .9929 .9931 .9932 .9934 .9936</p>
<p>2.5 .9938 .9940 .9941 .9943 .9945 .9946 .9948 .9949 .9951 .9952</p>
<p>2.6 .9953 .9955 .9956 .9957 .9959 .9960 .9961 .9962 .9963 .9964</p>
<p>2.7 .9965 .9966 .9967 .9968 .9969 .9970 .9971 .9972 .9973 .9974</p>
<p>2.8 .9974 .9975 .9976 .9977 .9977 .9978 .9979 .9979 .9980 .9981</p>
<p>2.9 .9981 .9982 .9982 .9983 .9984 .9984 .9985 .9985 .9986 .9986</p>
<p>3.0 .9987 .9987 .9987 .9988 .9988 .9989 .9989 .9989 .9990 .9990</p>
<p>3.1 .9990 .9991 .9991 .9991 .9992 .9992 .9992 .9992 .9993 .9993</p>
<p>3.2 .9993 .9993 .9994 .9994 .9994 .9994 .9994 .9995 .9995 .9995</p>
<p>3.3 .9995 .9995 .9995 .9996 .9996 .9996 .9996 .9996 .9996 .9997</p>
<p>3.4 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9998</p>
<p>Kaynaklar</p>
<p>[1] Gullickson, {\em Sociology G4075: Introduction to Social Data Analysis
  II}, <a href="https://web.archive.org/web/20160312151715/http://pages.uoregon.edu/aarong/teaching/G4075_Outline/node13.html">https://web.archive.org/web/20160312151715/http://pages.uoregon.edu/aarong/teaching/G4075_Outline/node13.html</a></p>
<hr>

<p>Yunan Harfleri</p>
<p><img alt="" src="../../algs/algs_999_zapp/letters.png" /></p>