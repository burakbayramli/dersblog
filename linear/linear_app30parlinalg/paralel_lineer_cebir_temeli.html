<!DOCTYPE html>
<html>
  <head>
    <title>Paralel Lineer Cebir Temeli
</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [["$","$"]]}
      });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    </script>
   <script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full">
   </script>
   <link rel="stylesheet" type="text/css" media="screen" href="https://burakbayramli.github.io/css/style.css">
  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1548953794786292"
          crossorigin="anonymous"></script>  
  </head>
    <body>
      <div id="header_wrap" class="outer">
        <header class="inner">
          <h1 id="project_title">
            <a href="https://burakbayramli.github.io" style="text-decoration:none; color:inherit;">dersblog</a>
          </h1>
          <h2 id="project_tagline"></h2>          
        </header>
      </div>
      <div id="main_content_wrap" class="outer">        
        <section id="main_content" class="inner">
        <h1>Paralel Lineer Cebir Temeli
</h1>

<p>Satırsal $A^TA$</p>
<p>[5]'te tek makinalı ortamda matris çarpımının nasıl yapılacağını, ve nasıl
görülecebileğini anlattık. Satır bakış açısı, kolon bakış açısı işlendi. Peki
parallel bir ortamda hangi matematik bizi ilgilendirmeli? Mesela $A^TA$'yi ele
alalım. Bu çarpım oldukça önemli çünkü başka sonuçlar için de kullanılabiliyor,
Eşsiz Değer Ayrıştırması (Singular Value Decomposition -SVD-) bunlardan biri.</p>
<p>Büyük Veri ve paralel işlem bağlamında $A^TA$'nin önemli şurada; eğer $A$
matrisi "uzun boylu ve zayıf" ise, yani çok miktarda satırı ama az miktarda
kolonu var ise, $m \times d$ diyelim, $A^TA$ çarpımı bize $d \times d$ boyutunda
ufak bir matris verir. Eğer SVD hesabını bu matris üzerinden yapabilirsek,
işimizi kolaylaştırmış oluruz. </p>
<p>Satırsal olarak $A^TA$ hesabı yapmak için satır satır gezerken her satırın
kendisi ile dışsal çarpımını (outer product) alıp sonuçları toplamak yeterli
[9]. İşlemi daha sözel tarif eden bir açıklama [3]'de bulunabilir.</p>
<p>$A^TA$ ve SVD</p>
<p>$A^TA$ açılımını yapalım [7], bir $A$ için SVD ayrıştırması</p>
<p>$$
A = U \Sigma V^T
$$</p>
<p>olduğuna göre</p>
<p>$$
A^TA =  (U \Sigma V^T)^T  U \Sigma V^T
$$</p>
<p>olacaktır, devam edersek,</p>
<p>$$
= V \Sigma U^T U \Sigma V^T 
$$</p>
<p>$U^T U = I$ olduğu için geri kalanlar</p>
<p>$$
A^TA = V \Sigma^2 V^T 
$$</p>
<p>Peki eşitliğin sağından $V$'yi nasıl çıkartırız? Bir dikgen matris çarpı köşegen
bir matris çarpı o dikgen matrisin devriği bize bir şeyi hatırlatıyor mu?  Evet,
bu bir özdeğer / özvektör hesabına benziyor, [2]'de görüldüğü gibi $A=S\Lambda
S^{-1}$ ayrıştırması da var (birimdik matrislerde tersi alma işleminin devrik
ile aynı şey olduğunu unutmayalım). O zaman $A^TA$'nin öz hesabını yaparsak
oradaki özvektör bize $V$ matrisini verecektir.</p>
<p>$U$'yi elde etmek için basit matris çarpımları yeterli,</p>
<p>$$
A = U \Sigma V^T \to U = A V \Sigma^{-1}
$$</p>
<p>Yani $V$ elde edildikten sonra onunla $A$'yi çarpıyoruz, sonra $\Sigma^{-1}$
ile. $A$ ne kadar büyük olursa olsun onu bir vektör $V$ ile çarpmak hızlı
işler, $\Sigma$ ise köşegen, seyrek bir matristir, onun çarpımı da basittir.</p>
<p>Bu işlemlerin paralel versiyonları için [8, 6] kaynaklarına bakılabilir.</p>
<p>SVD ve öz hesapların benzerliği için alttaki koda bakılabilir,</p>
<pre><code class="python">import numpy.linalg as lin
import pandas as pd

k = 7 # izdusum uzayinin boyutlari
df = pd.read_csv(&quot;../linear_app10rndsvd/w1.dat&quot;,sep=';',header=None)
A = np.array(df)[:,1:]
print (A.shape)
</code></pre>

<pre><code>(71, 30)
</code></pre>

<pre><code class="python">ATA = np.dot(A.T,A)
eval,evec = lin.eig(ATA)
u,s,v = lin.svd(A)
print (evec.shape)
print (v.shape)
</code></pre>

<pre><code>(30, 30)
(30, 30)
</code></pre>

<p>Matrisler birbirine değersel olarak ne kadar yakın, kontrol edelim,</p>
<pre><code class="python">print ( np.mean(np.abs(evec) - np.abs(v.T)))
</code></pre>

<pre><code>0.005230468666628483
-2.7259159635502234e-13
</code></pre>

<p>Fark çok ufak (<code>abs</code> ile mutlak değer kullandık çünkü bazen tüm bir kolon
diğerinin eksi hali olabiliyor).</p>
<p>SVD İçin QR</p>
<p>QR ile SVD yapmak mumkundur. QR ayrıştırması [4] kolonların hepsi bilindiği gibi
birbirine dik (orthogonal) birim vektörler olan bir $Q$ matrisi ve üst üçgensel
(upper triangular) bir $R$ matrisi oluşturur. Ayrıştırmanın $A^TA$ ile
bağlantısı nedir? Eğer $A$ yerine onun ayrıştırmasını $QR$ koyarsak,</p>
<p>$$
C = A^TA = (QR)^T (QR) = R^T Q^T QR
$$</p>
<p>Tum $Q$ vektorleri birbirine dik, ve birim vektorler ise, $Q^T Q$
birim matrisi $I$ olur. O zaman</p>
<p>$$
C = R^T Q^T QR = R^T R
$$</p>
<p>Yani</p>
<p>$$
C = R^TR
$$</p>
<p>Peki $A^TA$ hesaplayıp (böylece $R^TR$'yi elde edince) onun içinden $R$'yi nasıl
çekip çıkartırız? Şimdi Cholesky ayrıştırması kullanmanın zamanı. Cholesky
ayrıştırması (herhangi bir simetrik pozitif kesin $C$ matrisi üzerinde)</p>
<p>$$C = LL^T$$</p>
<p>olarak bilinir, yani bir matris alt üçgensel (lower triangular -ki L harfi
oradan geliyor-) $L$ matrisine ve onun devriği olan üst üçgensel $L^T$'nin
çarpımına ayrıştırılır. Elimizde $R^TR$ var, ve ona benzer $LL^T$ var, $R$
bilindiği gibi üst üçgensel, $L$ alt üçgensel, $L^T$ ve $R$ birbirine eşit demek
ki. Yani $A^TA$ üzerinde sayısal hesap kütüphenimzin Cholesky çağrısı kullanmak
bize $QR$'in $R$'sini verir.</p>
<p>Şu anda akla şu soru gelebilir: madem kütüphane çağrısı yaptık, niye $A$
üzerinde kütüphenimizin $QR$ çağrısını kullanmıyoruz?</p>
<p>Cevap Büyük Veri argümanında saklı. Bu ortamda uğraşılan verilerde $A$ matrisi
$m \times n$ boyutlarındadır, ve $m$ milyonlar, hatta milyarlarca satır
olabilir. Şimdilik $m &gt;&gt; n$ olduğunu farzedelim, yani $m$, $n$'den "çok, çok
büyük", yani "boyut kolonlarının", ki $n$, sayısı binler ya da onbinlerde. Bu
gayet tipik bir senaryo aslında, ölçüm noktaları (boyutlar) var, ama çok fazla
değil, diğer yandan o ölçümler için milyonlarca veri noktası toplanmış. Tipik
bir aşırı belirtilmiş (överdetermined) sistem - ki en az kareler (least squares)
gibi yaklaşımların temel aldığı sistemler bunlardır, eldeki denklem sayısından
daha fazla ölçüm noktası vardır. Bu arada en az karelerden bahsettik, $QR$'in
kullanıldığı alanlardan biri en az karelerin çözümüdür.</p>
<p>Argümana devam ediyoruz, kütüphane <code>qr</code> çağrısını $A$ üzerinde yaparsak, $m
\times n$ gibi devasa bir matris üzerinde işlem yapmak gerekir. Ama $A^TA$
üzerinde işlem (Cholesky) yaparsak, ki bu çarpımın boyutu $n \times m \cdot m
\times n = n \times n$, yani çok daha ufak bir matristir. $A^TA$'in işlem bedeli
çok ufak, birazdan anlatacağımız yöntem sayesinde bu bedel $O(m)$.</p>
<p>SVD</p>
<p>Simdi $QR$ sonuçlarını kullanarak SVD hesaplamaya gelelim. SVD bize ne verir?</p>
<p>$$ A = U \Sigma V^T $$</p>
<p>$U$ ve $V^T$ dikgen (orthogonal) matrislerdir, $\Sigma$ sadece köşegeni
boyunca değerleri olan bir matristir. Daha fazla detay için bkz [4]. Şimdi
$A = QR$ yerine koyalım,</p>
<p>$$ QR =  U \Sigma V^T $$</p>
<p>$$ R = Q^T U \Sigma V^T $$</p>
<p>Bu son formüledeki $Q^TU$ ibaresi, iki dikgen matrisin çarpımıdır. Lineer Cebir
kurallarına göre iki dikgen matrisin çarpımı bir diğer ortogonal matristir. Bu
yeni dikgen matrise $U_R$ adı verelim, o zaman</p>
<p>$$ R = U_R \Sigma V^T $$</p>
<p>Bu son formül bize bir şeyler söylüyor. $R$'nin SVD üzerinden
ayrıştırılabileceğini söylüyor ve bu ayrıştırma sonrası ele geçen $U_R,V^T$ ve
$\Sigma$ köşegen matrisleridir! Bu çok önemli bir sonuç.  Bu ayrıştırmanın
sonucu $A$'nin ki ile birbirine çok benziyor, tek fark $U$ ile $U_R$. Bu iki
matris arasındaki geçiş şöyle:</p>
<p>$$ U_R = Q^T U $$ </p>
<p>$$ U = QU_R $$ </p>
<p><img alt="" src="ur.png" /></p>
<p>Bu demektir ki eğer $R$ üzerinde kütüphanemizin <code>svd</code>  çağrısını
kullanırsak (ki $R$ nispeten ufak olduğu için bu ucuz olur) ele geçen $U_R$'i
alıp, $Q$ ile çarparsak, $A$ ayrıştırmasının $U$'sunu elde ederiz! $Q$ ile
paralel yapılabilir.</p>
<p>Tekrar paylaşmak gerekirse paralelleştirme teknikleri [6,8] yazılarında.</p>
<p>Kaynaklar</p>
<p>[1] Benson, A., <em>Tall-and-skinny Matrix Computations in MapReduce</em></p>
<p>[2] Bayramlı, <em>Lineer Cebir Ders 22</em>,
     <a href="https://burakbayramli.github.io/dersblog/linear/linear_22/ders_22.html">https://burakbayramli.github.io/dersblog/linear/linear_22/ders_22.html</a></p>
<p>[3] Bayramlı, <a href="https://burakbayramli.github.io/dersblog/sk/2022/11/paralel-lineer-cebir.html">https://burakbayramli.github.io/dersblog/sk/2022/11/paralel-lineer-cebir.html</a>     </p>
<p>[4] Bayramlı, Lineer Cebir, <em>Ders 29</em></p>
<p>[5] Bayramlı, Lineer Cebir, <em>Matris Çarpımı, Ders 1</em></p>
<p>[6] <a href="https://burakbayramli.github.io/dersblog/sk/2015/03/lineer-cebir-hadoop.html">Lineer Cebir ve Hadoop</a></p>
<p>[7] Zadeh, <em>CME 323: Distributed Algorithms and Optimization, Lecture 17</em>,
    <a href="https://stanford.edu/~rezab/classes/cme323/S17/">https://stanford.edu/~rezab/classes/cme323/S17/</a></p>
<p>[8] Bayramlı, <em>Paralel Lineer Cebir</em>,
    <a href="https://burakbayramli.github.io/dersblog/sk/2022/11/paralel-lineer-cebir.html">https://burakbayramli.github.io/dersblog/sk/2022/11/paralel-lineer-cebir.html</a></p>
<p>[9] Gundersen, <em>Matrix Multiplication as the Sum of Outer Products</em>,
    <a href="https://gregorygundersen.com/blog/2020/07/17/matmul/">https://gregorygundersen.com/blog/2020/07/17/matmul/</a></p>
          <br/><a href="../index.html">Yukarı</a>
        </section>          
      </div>
    </body>
</html>
