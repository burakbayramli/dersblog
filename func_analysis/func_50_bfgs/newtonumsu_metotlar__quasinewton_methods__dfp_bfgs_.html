<!DOCTYPE html>
<html>
  <head>
    <title>Newton-umsu Metotlar (Quasi-Newton Methods), DFP, BFGS 
</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [["$","$"]]}
      });
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"],
        bcancel: ["Extension","cancel"],
        xcancel: ["Extension","cancel"],
        cancelto: ["Extension","cancel"]
      });
    });
    </script>
   <script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full">
   </script>
   <link rel="stylesheet" type="text/css" media="screen" href="https://burakbayramli.github.io/css/style.css">
  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1548953794786292"
          crossorigin="anonymous"></script>  
  </head>
    <body>
      <div id="header_wrap" class="outer">
        <header class="inner">
          <h1 id="project_title">
            <a href="https://burakbayramli.github.io" style="text-decoration:none; color:inherit;">dersblog</a>
          </h1>
          <h2 id="project_tagline"></h2>          
        </header>
      </div>
      <div id="main_content_wrap" class="outer">        
        <section id="main_content" class="inner">
        <h1>Newton-umsu Metotlar (Quasi-Newton Methods), DFP, BFGS 
</h1>

<p>Bir $f$ hedef fonksiyonunun minimizasyonu için Newton metodunun özyineli 
algoritması</p>
<p>$$
x_{k+1} = x_k - F(x_k)^{-1} g_k
$$</p>
<p>ki $g$ gradyan, $F$ ise Hessian. </p>
<p>Ya da</p>
<p>$$
x_{k+1} = x_k - (\nabla^2 f(x_k))^{-1} \nabla f(x_k)
$$</p>
<p>Newton'umsu metotların ana fikri Hessian matrisi yerine sadece gradyan
bilgisini kullanarak yaklaşık bir $F_k$ kullanmak, diyelim ki $H_k$. Sonra
$f(\cdot)$'un karesel olarak temsilini yazalım, özyineli gidişat sırasında,
bir herhangi bir $x_{k+1}$ etrafında Taylor açılımı</p>
<p>$$
m_k(x) \equiv f(x_{k+1}) + \nabla f(x_{k+1})^T (x - x_{k+1}) + 
\frac{1}{2} (x - x_{k+1}) ^T H_{k+1}^T (x - x_{k+1}) 
$$</p>
<p>Eğer gradyanı alırsak </p>
<p>$$
\nabla m_k(x) = \nabla f(x_{k+1}) + H_{k+1}^{-1} (x-x_{k+1})
$$</p>
<p>Şimdi $k$ ve $k+1$ noktaları, gradyanları üzerinden bir $H^{k+1}$ ilişkisi
ortaya çıkartmak istiyoruz ki çözüp bir sonuç elde edebilelim. Ek
denklemler elde etmek için şu akla yatkın şartları öne sürebiliriz, $m$ ve
$f$ gradyanları birbirine uysun. Yani,</p>
<p>$$
\nabla m_k(x) = \nabla f(x_k)
$$</p>
<p>O zaman, "Newton-umsuluk şartı (quasi-Newton condition)" da denen iki
üstteki denklemle beraber, ve açılımda $x$ herhangi bir $x$ olabileceği
için onun yerine $x_k$ kullanarak,</p>
<p>$$
\nabla f(x_{k+1}) + H_{k+1}^{-1} (x_k-x_{k+1}) = \nabla f(x_k)
$$</p>
<p>$$
H_{k+1}^{-1} (x_k-x_{k+1}) = \nabla f(x_k) - \nabla f(x_{k+1}) 
$$</p>
<p>$$
H_{k+1}^{-1} (x_{k+1}-x_k) = \nabla f(x_{k+1}) - \nabla f(x_k) 
$$</p>
<p>Üsttekine sekant denklemi adı veriliyor, şu figürle alakalı, </p>
<p><img alt="" src="func_50_bfgs_01.png" /></p>
<p>Yani sekant denklemine göre $H_{k+1}^{-1}$ değeri, yatay kordinattaki
$x^{k+1}-x^k$ değişimini, gradyan değişimi $\nabla f(x^{k+1}) - \nabla
f(x^k)$'e taşıyor / eşliyor [4].</p>
<p>Kısaltma amaçlı,</p>
<p>$$
H_{k+1}^{-1} \underbrace{(x_{k+1}-x_k)}<em>{y_k} = 
\underbrace{\nabla f(x</em>{k+1}) - \nabla f(x_k)}_{s_k}
$$</p>
<p>$$
H_{k+1}^{-1} y_k = s_k
\qquad (1)
$$</p>
<p>Özyineli bağlamda bir $H_0$'dan başlayarak ufak değişimlerle sonuca
ulaşılmaya uğraşılır. Değişimlerin ufak olması gerekliliği üzerinden ve bu
değişimlerin kerte 1 eki ile olması sonucu [4]'teki matris normu ile
beraber aslında birazdan türeteceğimiz güncelleme denklemi
alınabiliyor. Kerte 1 eki konusu için bkz [5]. Biz farklı bir yönden, eğer
ufak değişim kerte 1 ve 2 ile yapılsa nereye varılacağına bakacağız [1,
sf. 111].</p>
<p>Kerte 1 eki ile $H_k$'yi $H_{k+1}$ yapmak demek aslında</p>
<p>$$
H_{k+1} = H_k + czz^T 
$$</p>
<p>demektir. Bunu iki üstteki formül içine koyarsak</p>
<p>$$
s_k = (H_k + czz^T) y_k = H_k y_k + cz (z^T y_k)
$$</p>
<p>$z^T y_k$ bir skalar olduğu için </p>
<p>$$
cz = \frac{s_k - H_k y_k}{z^T y_k}
$$</p>
<p>Bu denklemi çözen en basit $c,z$ seçenekleri</p>
<p>$$
z = s_k - H_ky_k
$$</p>
<p>$$
c = \frac{1}{z^Ty_k}
$$</p>
<p>Bu bize kerte 1 güncelleme formülünü verir,</p>
<p>$$
H_{k+1} = H_k + \frac{ (s-H_ky_k) (s-H_ky_k)^T }{(s-H_ky_k) y_k}
$$</p>
<p>Ne yazık ki kerte 1 güncelemesinin bazı problemleri var. Bunlardan en
önemlisi güncelleme sonrası elde edilen yeni $H_{k+1}$'in pozitif kesin
olmasının garanti olmaması, bu sebeple bir sonraki döngüde elde edilecek
yön $d_k = -H_k \nabla f(x_k)$'nin bir iniş yönü olmasının garantisinin de
tehlikeye girmesi. </p>
<p>Çözüm olarak $H_{k+1}$'in pozitif kesin kalmasını garantileyecek kerte 2
güncellemesi keşfedilmiştir. Yani</p>
<p>$$
H_{k+1} = H_k + c_1z_1z_1^T + c_2z_2z_2^T 
$$</p>
<p>Pozitif kesinliğin ispatı için [2, sf. 206].</p>
<p>Yine (1)'deki Newton-umsuluk şartıyla beraber</p>
<p>$$
s_k = H_k y_k + c_1 z_1 (z_1^Ty_k) + c_2z_2 (z_2^Ty_k)
$$</p>
<p>$z_1$ ve $z_2$ için özgün çözüm olmamasına rağmen üstteki denklemi tatmin
edecek seçenekler bulunabilir,</p>
<p>$$
z_1 = s_k, \quad 
z_2 = H_k y_k, \quad
c_1 = \frac{1}{z_1^Ty_k}, \quad
c_2 = \frac{1}{z_2^Ty_k}
$$</p>
<p>Ve böylece kerte 2 güncellemesi şu hale gelir, </p>
<p>$$
H_{k+1} = H_k + \frac{y_ky_k^T}{s_k^Ty_k} - \frac{(H_k y_k)(H_k y_k)^T}{(H_ky_k)^Ty_k}
$$</p>
<p>Bu formüle Davidon-Fletcher-Powell (DFP) formülü adı verilir. </p>
<p>Algoritma şöyle</p>
<p>1) $k=0$ yap. Bir $x_0$'dan başla, ve herhangi bir simetrik, pozitif kesin
bir $H_0$ al</p>
<p>2) Eğer $s_k = 0$ ise dur, yoksa $d_k = -H_k g_k$</p>
<p>3) Şunu hesapla</p>
<p>$$
\alpha_k = \arg\min_{\alpha \ge 0} f(x_k + \alpha d_k)
$$</p>
<p>$$
x_{k+1} = x_k + \alpha_k d_k
$$</p>
<p>4) Hesapla</p>
<p>$$
y_k = \alpha_k d_k
$$</p>
<p>$$
s_k = g_{k+1} - g_k
$$</p>
<p>$$
H_{k+1} = H_k + \frac{y_ky_k^T}{s_k^Ty_k} - \frac{(H_k y_k)(H_k y_k)^T}{(H_ky_k)^Ty_k}
$$</p>
<p>BFGS</p>
<p>DFP ile kerte 2 güncellemesi oluyor böylece $H_{k+1}$ pozitif kesin
kalıyor, güzel. Fakat DFP'nin hala sayısal olarak bazı problemleri
var. Burada problem Hessian'ın değil Hessian'ın tersinin yaklaşıklamasının
güncelleniyor olması. Daha iyi bir seçim Hessian'ın <em>kendisinin</em>
yaklaşıklamasının güncellenmesi ve onun üzerinden bir terslik elde
edilmesi olmaz mıydı? Evet.</p>
<p>Devam etmeden önce işimize yarayacak başka bir konu, ikizlik konusundan
bahsedelim. Eğer DFP formülünün tersinin alırsak belli bir sonuç elde
ederiz (bunun benzerini yapacağız). Ama biz bu noktaya (1)'deki</p>
<p>$$
H_{k+1} y_k = s_k
$$</p>
<p>ile geldiğimizi biliyoruz, ve üstteki formülde ufak bir takla atarsak</p>
<p>$$
y_k = B_{k+1} s_k
$$</p>
<p>sonucuna gelebileceğimizi de biliyoruz, ki $B_k$, $F_k$'nin yaklaşık
hali. Dikkat edersek bu yeni Newton-umsuluk kuralı form olarak bir öncekine
çok benziyor, sadece $H_k$ yerine $B_k$ var ve $y_k,s_k$ yerleri değişti!
Bundan istifade edebiliriz, ve şimdiye kadar yapılan tüm türetme
işlemlerini kullanarak ve sadece $y_k,s_k$ yerini değiştirerek $B_k$ için
bir güncelleme formülü elde edebiliriz.</p>
<p>$$
B_{k+1} = B_k + \frac{s_ks_k^T}{y_k^Ts_k} - \frac{(B_k s_k)(B_k s_k)^T}{(B_ks_k)^Ts_k}
$$</p>
<p>İşte $B_k$'nin BFGS güncellemesi budur, isim Broyden, Fletcher, Goldfarb,
and Shannon adlı araştırmacılardan geliyor. Şimdi <em>üsttekinin tersini</em>
alırsak arka planda yapılan ve daha stabil olan $H_k$'nin güncellenmesinden
faydalanmış oluyoruz, ama hala her adımda bizim ilgilendiğimiz matris
tersine erişmiş oluyoruz. Üstteki formülün sağ tarafının tersi için
[6]'daki Sherman-Morrison tekniğini kullanacağız. SM formülü neydi?</p>
<p>$$
(A+uv^T)^{-1} = A ^{-1} - \frac{(A^{-1} u)(v^TA^{-1})}{1 + v^T A^{-1} u}
$$</p>
<p>eğer $1+v^TA ^{-1} y \ne 0$ ise.</p>
<p>Şimdi eğer ana güncelleme formülünü </p>
<p>$$
B_{k+1} = A_0 + u_0v_0^T + u_1v_1^T
$$</p>
<p>formuna getirebilirsek SM kullanabiliriz. Şu eşitlikleri kullanalım, </p>
<p>$$
A_0 = B_k, \quad u_o = \frac{s_k}{s_k^Ty_k}, \quad v_0^T = s_k^T
$$</p>
<p>$$
A_1 = B_k + \frac{s_k s_k^T}{s_k^Ty_k} = A_0 + u_0v_0^T, \quad 
u_1 = -\frac{B_k y_k}{y_k^TB_ky_k}
$$</p>
<p>$$
v_1^T = y_k^T B_k
$$</p>
<p>Böylece </p>
<p>$$
B_{k+1} = A_0 + u_0v_0^T + u_1v_1^T
$$</p>
<p>formülüne erişmiş olduk. Bu $B_{k+1}$ üzerinden bir ters elde etmek için,
ki bu sonuca $H_{k+1}^{BFGS}$ diyelim, </p>
<p>$$
H_{k+1}^{BFGS} = B_{k+1}^{-1} 
$$</p>
<p>$$
= (A_1 + u_1v_1^T)^{-1} 
$$</p>
<p>SM açılımına göre,</p>
<p>$$
= A_1^{-1} - \frac{A_1^{-1}u_1v_1^TA_1^{-1}}{1+v_1^TA_1^{-1}u_1 }
$$</p>
<p>$A_1^{-1}$ de SM ile açılacak tabii (onun için bu $A_1$'i belli bir forma
getirdik)</p>
<p>$$
H_{k+1}^{BFGS} = 
A_0^{-1} - \frac{A_0^{-1} u_0v_0^T A_0^{-1}}{1+v_0^TA_0^{-1}u_0 } -
\frac
  {
    (A_0^{-1} - \frac{A_0^{-1} u_0v_0^T A_0^{-1}}{1+v_0^TA_0^{-1}u_0})
     u_1v_1^T
    (A_0^{-1} - \frac{A_0^{-1} u_0v_0^T A_0^{-1}}{1+v_0^TA_0^{-1}u_0})
   }
  {1 + v_1^T (A_0^{-1} - \frac{A_0^{-1} u_0v_0^T A_0^{-1}}{1+v_0^TA_0^{-1}u_0})u_1}
$$</p>
<p>Dikkat edersek $A_0 = B_k$. O zaman $A_0^{-1} = B_k^{-1} = H_k$. Bu
eşitliği ve ilk başta gösterdiğimiz notasyonu kullanarak, </p>
<p>$$
H_{k+1}^{BFGS} = H_k - \frac{H_ks_ks_k^TH_k}{y_k^Ts_k + s_k^TH_ks_k} -
\frac
  {(H_k - \frac{H_ks_ks_k^TH_k}{y_k^Ts_k + s_k^TH_ks_k})
    (\frac{-B_k y_ky_k^TB_k}{y_k^TB_ky_k}) 
  }
  {1+y_k^TB_k (H_k - \frac{H_ks_ks_k^TH_k}{y_k^Ts_k + s_k^TH_ks_k})(\frac{-B_k y_ky_k^TB_k}{y_k^TB_ky_k})}
$$
$$
\times (H_k - \frac{H_ks_ks_k^TH_k}{y_k^Ts_k + s_k^TH_ks_k})
$$</p>
<p>Bazı çarpımları yaptıktan sonra ve $H_k = B_k^{-1}$ olduğunu hesaba
katarak, yani</p>
<p>$$
H_kB_k = B_kH_k=I_n
$$</p>
<p>diyerek, alttakini elde ediyoruz,</p>
<p>$$
H_{k+1}^{BFGS} = H_k - \frac{H_ks_ks_k^TH_k}{s_k^Ty_k + s_k^TH_ks_k} -
\frac
{ 
  (1 - \frac{H_k s_ks_k^T}{s_k^T y_k + s_k^TH_ks_k})
  (-y_ky_k^T)
  (1 - \frac{s_ks_k^TH_k}{s_k^Ty_k + s_k^TH_ks_k})
}
{y_k^T B_k y_k + y_k^T (B_k - \frac{s_k^Ts_k}{s_k^Ty_k + s_k^TH_ks_k})(-y_k)  }
$$</p>
<p>Sembolik işlemlerimize devam ediyoruz. $y_k$ ve $y_k^T$ çarparak alttakini
elde ediyoruz, </p>
<p>$$
H_{k+1}^{BFGS} =  H_k - \frac{H_ks_ks_k^TH_k}{s_k^Ty_k + s_k^TH_ks_k} - 
\frac
{ 
  (\frac{H_k s_ks_k^Ty_k}{s_k^T y_k + s_k^TH_ks_k} - y_k)
  (x_k^T - \frac{y_k^T s_ks_k^TH_k}{s_k^Ty_k + s_k^TH_ks_k})
}
{
y_k^TB_ky_k - y_k^T B_k y_k + 
\frac{y_k^T s_ks_k^Ty_k}{s_k^Ty_k + s_k^TH_ks_k}
}
$$</p>
<p>Üstte en son terimdeki bölendeki terimleri iptal edince ve daha fazla
çarpma işlemi yapınca,</p>
<p>$$
H_{k+1}^{BFGS} =  H_k - 
\frac{H_ks_ks_k^TH_k}{s_ky_k + s_k^TH_ks_k} +
\frac
   {\frac{H_k s_k(s_k^T y_k) (y_k^T s_k)s_k H_k }{s_k^T y_k + s_k^T H_k s_k}}
   {(y_k^T s_k)(s_k^T y_k)} +
\frac
   {y_k y_k^T (s_k^T y_k + s_k^T H_k s_k) }
   {(y_k^T s_k)(s_k^T y_k)} - 
$$
$$
\frac
   {H_k s_k (s_k^T y_k) y_k^T + y_k s_k^T H_k  }
   {(y_k^T s_k)(s_k^Ty_k)}
$$</p>
<ol>
<li>ve 5. terimlerde daha da basitleştirme yapınca</li>
</ol>
<p>$$
H_{k+1}^{BFGS} =<br />
H_k - \frac{H_k s_k s_k^T H_k}{s_k^T y_k + s_k^T H_k s_k} +
\frac{H_k s_k s_k^T H_k}{s_k y_k + s_k^T H_k s_k} +
\frac{y_k y_k^T (s_k^T y_k + s_k^T H_k s_k)}{(y_k^T s_k)(s_k^Ty_k)} - 
\frac{H_k s_k y_k^T + y_ks_k^T H_k}{y_k^T s_k}
$$</p>
<p>Dikkat edersek 2. ve 3. terimleri birbirini iptal ediyor, o zaman, ve
4. terimi alternatif bir formda gösterirsek,</p>
<p>$$
H_{k+1}^{BFGS} =  H_k + 
\frac{y_k y_k^T}{y_k^T s_k} \left( 1 + \frac{s_k^T H_k s_k}{s_k^T y_k}  \right) - 
\frac{H_k s_k y_k^T + y_k s_k^T H_k}{y_k^T s_k}
$$</p>
<p>Nihai BFGS formülüne erişmiş olduk. Bu formülü alttaki gibi de
gösterebiliriz [7],</p>
<p>$$
H_{k+1}^{BFGS} =<br />
\left( I - \frac{s_k y_k^T}{s_k y_k} \right) 
H_k
\left( I - \frac{s_k y_k^T}{y_k^T s_k} \right) + 
\frac{y_k y_k^T}{y_k^T s_k}
$$</p>
<p>Bir örnek üzerinde görelim,</p>
<pre><code class="python">import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import numpy.linalg as lin

eps = np.sqrt(np.finfo(float).eps)

def rosen(x):
    return 100*(x[1]-x[0]**2)**2+(1-x[0])**2

def rosen_real(x):
    gy =[-400*(x[1]-x[0]**2)*x[0]-2*(1-x[0]), 200*(x[1]-x[0]**2)]
    return rosen(x), gy

def linesearch_secant(f, d, x):
    epsilon=10**(-5)
    max = 500
    alpha_curr=0
    alpha=10**-5
    y,grad=f(x)
    dphi_zero=np.dot(np.array(grad).T,d)

    dphi_curr=dphi_zero
    i=0;
    while np.abs(dphi_curr)&gt;epsilon*np.abs(dphi_zero):
        alpha_old=alpha_curr
        alpha_curr=alpha
        dphi_old=dphi_curr
        y,grad=f(x+alpha_curr*d)
        dphi_curr=np.dot(np.array(grad).T,d)
        alpha=(dphi_curr*alpha_old-dphi_old*alpha_curr)/(dphi_curr-dphi_old);
        i += 1
        if (i &gt;= max) and (np.abs(dphi_curr)&gt;epsilon*np.abs(dphi_zero)):
            print('Line search terminating with number of iterations:')
            print(i)
            print(alpha)
            break

    return alpha

def bfgs(x, func):

    H = np.eye(2)
    tol = 1e-20
    y,grad = func(x)
    dist=2*tol
    epsilon = tol
    iter=0;

    while lin.norm(grad)&gt;1e-6:
        value,grad=func(x)
        p=np.dot(-H,grad)
        lam = linesearch_secant(func,p,x)
        iter += 1
        xt = x
        x = x + lam*p
        s = lam*p
        dist=lin.norm(s)
        newvalue,newgrad=func(x)
        y = np.array(newgrad)-grad
        rho=1/np.dot(y.T,s)
        s = s.reshape(2,1)
        y = y.reshape(2,1)
        tmp1 = np.eye(2)-rho*np.dot(s,y.T)
        tmp2 = np.eye(2)-rho*np.dot(y,s.T)
        tmp3 = rho*np.dot(s,s.T)
        H= np.dot(np.dot(tmp1,H),tmp2) + tmp3
        #print ('lambda:',lam)

    print (xt)
    print ('iter',iter)
</code></pre>

<pre><code class="python">x=np.array([-1.0,0])
bfgs(x,rosen_real)    
</code></pre>

<pre><code>[1. 1.]
iter 19
</code></pre>

<p>Eğer gradyan yerine yaklaşıksal gradyan hesap fonksiyonunu kullanırsak,</p>
<pre><code class="python">def _approx_fprime_helper(xk, f, epsilon):
    f0 = f(xk)
    grad = np.zeros((len(xk),), float)
    ei = np.zeros((len(xk),), float)
    for k in range(len(xk)):
        ei[k] = 1.0
        d = epsilon * ei
        df = (f(xk + d) - f0) / d[k]
        if not np.isscalar(df):
            try:
                df = df.item()
            except (ValueError, AttributeError):
                raise ValueError(&quot;The user-provided &quot;
                                 &quot;objective function must &quot;
                                 &quot;return a scalar value.&quot;)
        grad[k] = df
        ei[k] = 0.0
    return grad


def rosen_approx(x):
    g = _approx_fprime_helper(x, rosen, eps)
    return rosen(x),g

bfgs(x,rosen_approx)
</code></pre>

<pre><code>[0.99999552 0.99999104]
iter 19
</code></pre>

<p>yine optimum noktaya erişmiş oluyoruz.</p>
<p>Yakınsaklık garantileri açısından, Newton-umsu metotlar her adımda bir
pozitif kesin $H_k$ ürettikleri için çizgi aramasıyla birleştirilmiş normal
Newton metotlarıyla aynı şekilde sürekli iniş özelliğine sahip olacaktır,
bu sebeple 1. derecede optimallik şartı açısından, nereden başlanırsa
başlansın bir minimuma ulaşacaklardır. Detaylar için [2].</p>
<p>Kaynaklar </p>
<p>[1] Dutta, <em>Optimization in Chemical Engineering</em></p>
<p>[2] Zak, <em>An Introduction to Optimization, 4th Edition</em></p>
<p>[3] Bayramlı, <em>Hesapsal Bilim, Sayısal Entegrasyon ve Sonlu Farklılıklar ile Sayısal Türev</em></p>
<p>[4] Chen, <em>ELE522 - Large Scale Optimization Lecture, Princeton</em>,
    <a href="http://www.princeton.edu/~yc5/ele522_optimization/">http://www.princeton.edu/~yc5/ele522_optimization/</a></p>
<p>[5] Bayramlı, <em>Lineer Cebir, Ders 8, Kerte Konusu</em></p>
<p>[6] Bayramlı, <em>Lineer Cebir, Ekler, Sherley-Morrison Formülü</em></p>
<p>[7] Fletcher, <em>A new approach to variable metric problems</em></p>
        </section>          
      </div>
    </body>
</html>
